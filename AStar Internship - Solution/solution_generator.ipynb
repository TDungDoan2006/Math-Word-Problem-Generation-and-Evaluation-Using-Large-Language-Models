{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9c4948-c928-4286-95bb-0894c64b88c9",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11929bb-0d51-4909-95d1-c776b7a059b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important libraries\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "from openai import OpenAI\n",
    "\n",
    "from site_wrapper.openrouter import OpenRouter\n",
    "from site_wrapper.platform_migration.api_utils import Azure\n",
    "from site_wrapper.platform_migration.api_utils import Aws\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict\n",
    "\n",
    "file_id = \"2025-06-09_14_16_30\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163626b9-7aee-48ae-abfd-a668e51c8583",
   "metadata": {},
   "source": [
    "### Import original PSLE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bae520b-6d8f-4e17-8d28-032121eb638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./dataset/{file_id}_generated_questions.json\", \"r\") as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f8f1d-ff6d-40a1-a876-fa08cb41f9ca",
   "metadata": {},
   "source": [
    "# Initialize keys and token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cd32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"openai/gpt-4.1\", \"meta-llama/llama-4-scout\", \"deepseek/deepseek-r1\", \n",
    "          \"microsoft/phi-4\", \"qwen/qwen3-14b\", \"anthropic/claude-3.7-sonnet\"]\n",
    "models_azure = [\"openai/gpt-4.1\", \"meta-llama/llama-4-scout\", \"deepseek/deepseek-r1\", \"microsoft/phi-4\"]\n",
    "models_aws = [\"anthropic/claude-3.7-sonnet\"]\n",
    "models_platform_translation = {\"openai/gpt-4.1\" : \"gpt-4.1\", \"deepseek/deepseek-r1\" : \"DeepSeek-R1-0528\", \"microsoft/phi-4\" : \"Phi-4\",\n",
    "                               \"meta-llama/llama-4-scout\" : \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"qwen/qwen3-14b\" : \"qwen/qwen3-14b\",\n",
    "                               \"anthropic/claude-3.7-sonnet\" : \"apac.anthropic.claude-sonnet-4-20250514-v1:0\"}\n",
    "models_open_router = [\"qwen/qwen3-14b\"]\n",
    "models_free = [\"microsoft/phi-4-reasoning-plus:free\", \"deepseek/deepseek-r1-distill-qwen-32b:free\", \"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "              \"qwen/qwen3-32b:free\"]\n",
    "model_default = \"deepseek/deepseek-r1-distill-qwen-32b:free\"\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c1594-aaff-48c9-ac7c-21fe7fd704d2",
   "metadata": {},
   "source": [
    "# Main LLM generating correct solution loop\n",
    "\n",
    "This code let each of the LLM solve PSLE questions and store those answers in the ```solutions``` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902df45f-039e-43f6-a97a-ba11b103ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working with model:  openai/gpt-4.1\n",
      "Knowledge component P1N100 (1/32) loaded successfully\n",
      "{'question': 'Sarah has 37 stickers and Ben has 48 stickers. How many stickers do they have altogether?', 'solution': {'Step 1': \"We need to find the total number of stickers by adding Sarah's and Ben's stickers.\", 'Step 2': 'Sarah has 37 stickers. Ben has 48 stickers.', 'Step 3': 'Add the two numbers together: \\n\\n\\\\[ 37 + 48 = 85 \\\\]', 'Step 4': 'So, Sarah and Ben have 85 stickers altogether.'}}\n",
      "Knowledge component P1AddSub (2/32) loaded successfully\n",
      "{'question': 'Amir has 8 toy cars. His friend gives him 6 more toy cars. How many toy cars does Amir have now?', 'solution': {'Step 1': 'First, we find out how many toy cars Amir starts with. Amir has 8 toy cars.', 'Step 2': 'Next, we see how many toy cars his friend gives to him. His friend gives him 6 more toy cars.', 'Step 3': 'To find the total number of toy cars Amir has now, we need to add the toy cars he started with and the toy cars his friend gave him.', 'Step 4': 'We use the addition method: 8 + 6.', 'Step 5': '8 + 6 = 14.', 'Step 6': 'Amir now has 14 toy cars in total.'}}\n",
      "Knowledge component P1MulDiv (3/32) loaded successfully\n",
      "{'question': 'Each packet contains 4 apples. Mrs Lim bought 3 packets of apples. How many apples did she buy altogether?', 'solution': {'Step 1': 'First, find out how many apples are in 1 packet. There are 4 apples in 1 packet.', 'Step 2': 'Mrs Lim bought 3 packets. So, we need to find out how many apples are in 3 packets.', 'Step 3': 'To do this, we multiply the number of apples in 1 packet by the number of packets: \\n\\n\\\\[ 4 \\\\times 3 = 12 \\\\]', 'Step 4': 'So, Mrs Lim bought 12 apples altogether.'}}\n",
      "Knowledge component P1Money (4/32) loaded successfully\n",
      "{'question': 'Shawn has a 50-cent coin and two 20-cent coins. How much money does he have altogether?', 'solution': {'Step 1': 'First, recognise the values of the coins. Shawn has one 50-cent coin and two 20-cent coins.', 'Step 2': 'Next, add the value of the two 20-cent coins: 20 cents + 20 cents = 40 cents.', 'Step 3': 'Now, add this to the 50-cent coin: 50 cents + 40 cents = 90 cents.', 'Step 4': 'So, Shawn has 90 cents altogether.'}}\n",
      "Knowledge component P2N1000 (5/32) loaded successfully\n",
      "{'question': 'Maya has 325 stickers. Her brother gives her 250 more stickers. How many stickers does Maya have in total now?', 'solution': {'Step 1': 'Understand the problem. Maya starts with 325 stickers. Her brother gives her 250 more stickers.', 'Step 2': 'To find out how many stickers Maya has in total, add the number of stickers she had at first to the number of stickers her brother gave her.', 'Step 3': 'Write the addition equation: \\n\\n\\\\[ 325 + 250 \\\\]', 'Step 4': 'Add the hundreds, tens, and ones separately:\\nHundreds: 3 + 2 = 5\\nTens: 2 + 5 = 7\\nOnes: 5 + 0 = 5\\nSo, \\n\\n\\\\[ 325 + 250 = 575 \\\\]', 'Step 5': 'So, Maya has 575 stickers in total now.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\['\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/taxonomy definitions/BLOOM.txt', 'r', encoding='utf-8', errors='replace') as f:\n",
    "    taxonomy_definition = f.read()\n",
    "for model in models[:1]:\n",
    "    kc_index = 0\n",
    "    dataset_length = len(dataset.keys())\n",
    "\n",
    "    print(\"Currently working with model: \", model)\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "    system_prompt_fpath = f\"./prompt/correct solution/{model_path}_system_prompt.txt\"\n",
    "    user_prompt_fpath = f\"./prompt/correct solution/{model_path}_user_prompt.txt\"\n",
    "    generated_solution_fpath = f\"./solution/correct solution/dataset_{file_id}/{model_path}_solution.json\"\n",
    "\n",
    "    all_outputs_dct = {}\n",
    "    all_outputs_text = {}\n",
    "    errors = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for kcID in list(dataset.keys())[:5]:\n",
    "        kc_index += 1\n",
    "        info = {\n",
    "            \"id\" : dataset[kcID][\"ID\"],\n",
    "            \"question\" : dataset[kcID][\"Question\"],\n",
    "            \"taxonomy definition\" : taxonomy_definition,\n",
    "            \"answer\" : None,\n",
    "            \"kc\": None\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(system_prompt_fpath,\"r\") as file:\n",
    "                system_prompt = file.read()\n",
    "        except Exception as e:\n",
    "            system_prompt = \"\"\n",
    "        try:\n",
    "            with open(user_prompt_fpath,\"r\") as file:\n",
    "                user_prompt = file.read()\n",
    "        except Exception as e:\n",
    "            user_prompt = \"\"\n",
    "\n",
    "        this_system_prompt = system_prompt.format()\n",
    "        this_user_prompt = user_prompt.format(question = info[\"question\"], ID = info[\"id\"], taxonomy_definition = info[\"taxonomy definition\"])\n",
    "\n",
    "        if model in models_azure:\n",
    "            wrapper = Azure()\n",
    "        elif model in models_aws:\n",
    "            wrapper = Aws()\n",
    "        else:\n",
    "            wrapper = OpenRouter()\n",
    "\n",
    "        if model == \"openai/gpt-4.1\":\n",
    "            message = wrapper.get_GPT_txt_response(this_system_prompt, this_user_prompt)\n",
    "        else:\n",
    "            message = wrapper.get_LLM_response(models_platform_translation[model], this_system_prompt, this_user_prompt)\n",
    "    \n",
    "        response_text = message.strip()\n",
    "        if model == \"deepseek/deepseek-r1\":\n",
    "            _,__,response_text = response_text.partition(\"</think>\")\n",
    "            response_text = response_text.strip()\n",
    "\n",
    "        response_text_extracted = None\n",
    "        match = re.findall(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_text_extracted = match[0]\n",
    "        else:\n",
    "            print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "            print(response_text)\n",
    "    \n",
    "        try:\n",
    "            response_dct = ast.literal_eval(response_text_extracted)\n",
    "            all_outputs_dct[kcID] = response_dct\n",
    "        except Exception as e:\n",
    "            errors[kcID] = response_text\n",
    "            print(\"Found error at\", kcID, \"error: \", e)\n",
    "            print(response_text)\n",
    "\n",
    "        all_outputs_text[kcID] = response_text\n",
    "\n",
    "\n",
    "        print(f\"Knowledge component {kcID} ({kc_index}/{dataset_length}) loaded successfully\")\n",
    "        print(response_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fd580b-0016-4427-8544-b1e6005ac25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': '', 'status': -1, 'response_time': 2.7644898891448975}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ab5471a-a7f9-4ff8-bc60-772c25b8326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P1N100': {'question': 'Sarah has 37 stickers and Ben has 48 stickers. How many stickers do they have altogether?',\n",
       "  'solution': {'Step 1': \"We need to find the total number of stickers by adding Sarah's and Ben's stickers.\",\n",
       "   'Step 2': 'Sarah has 37 stickers. Ben has 48 stickers.',\n",
       "   'Step 3': 'Add the two numbers together: \\n\\n\\\\[ 37 + 48 = 85 \\\\]',\n",
       "   'Step 4': 'So, Sarah and Ben have 85 stickers altogether.'}},\n",
       " 'P1AddSub': {'question': 'Amir has 8 toy cars. His friend gives him 6 more toy cars. How many toy cars does Amir have now?',\n",
       "  'solution': {'Step 1': 'First, we find out how many toy cars Amir starts with. Amir has 8 toy cars.',\n",
       "   'Step 2': 'Next, we see how many toy cars his friend gives to him. His friend gives him 6 more toy cars.',\n",
       "   'Step 3': 'To find the total number of toy cars Amir has now, we need to add the toy cars he started with and the toy cars his friend gave him.',\n",
       "   'Step 4': 'We use the addition method: 8 + 6.',\n",
       "   'Step 5': '8 + 6 = 14.',\n",
       "   'Step 6': 'Amir now has 14 toy cars in total.'}},\n",
       " 'P1MulDiv': {'question': 'Each packet contains 4 apples. Mrs Lim bought 3 packets of apples. How many apples did she buy altogether?',\n",
       "  'solution': {'Step 1': 'First, find out how many apples are in 1 packet. There are 4 apples in 1 packet.',\n",
       "   'Step 2': 'Mrs Lim bought 3 packets. So, we need to find out how many apples are in 3 packets.',\n",
       "   'Step 3': 'To do this, we multiply the number of apples in 1 packet by the number of packets: \\n\\n\\\\[ 4 \\\\times 3 = 12 \\\\]',\n",
       "   'Step 4': 'So, Mrs Lim bought 12 apples altogether.'}},\n",
       " 'P1Money': {'question': 'Shawn has a 50-cent coin and two 20-cent coins. How much money does he have altogether?',\n",
       "  'solution': {'Step 1': 'First, recognise the values of the coins. Shawn has one 50-cent coin and two 20-cent coins.',\n",
       "   'Step 2': 'Next, add the value of the two 20-cent coins: 20 cents + 20 cents = 40 cents.',\n",
       "   'Step 3': 'Now, add this to the 50-cent coin: 50 cents + 40 cents = 90 cents.',\n",
       "   'Step 4': 'So, Shawn has 90 cents altogether.'}},\n",
       " 'P2N1000': {'question': 'Maya has 325 stickers. Her brother gives her 250 more stickers. How many stickers does Maya have in total now?',\n",
       "  'solution': {'Step 1': 'Understand the problem. Maya starts with 325 stickers. Her brother gives her 250 more stickers.',\n",
       "   'Step 2': 'To find out how many stickers Maya has in total, add the number of stickers she had at first to the number of stickers her brother gave her.',\n",
       "   'Step 3': 'Write the addition equation: \\n\\n\\\\[ 325 + 250 \\\\]',\n",
       "   'Step 4': 'Add the hundreds, tens, and ones separately:\\nHundreds: 3 + 2 = 5\\nTens: 2 + 5 = 7\\nOnes: 5 + 0 = 5\\nSo, \\n\\n\\\\[ 325 + 250 = 575 \\\\]',\n",
       "   'Step 5': 'So, Maya has 575 stickers in total now.'}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "391c7adb-19bc-40b7-be1e-02754dabc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./solution/correct solution/td_questions/{model_path}_solution.json', 'w') as file:\n",
    "    json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a9d3cdd-7fbb-4e92-bef7-e9aa84212b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_latex_string(s):\n",
    "    s = s.replace('\\n', ' \\n')   # Replace \\n with LaTeX newline command\n",
    "    s = s.replace('$', '\\\\$')\n",
    "    s = s.replace('%', '\\\\%')\n",
    "    s = s.replace('\\\\\\\\', '\\\\')   # Normalize double backslashes\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "640698a6-3116-4ae6-83e5-4d419831214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us solve the problem step by step. \n",
      " \n",
      "First, Aisha has 256 stickers. \n",
      " \n",
      "She buys another 137 stickers. To find the total number of stickers she has after buying more, we add: \n",
      " \n",
      "\\[ \n",
      "256 + 137 = 393 \n",
      "\\] \n",
      " \n",
      "Now, Aisha gives 89 stickers to her friend. To find out how many stickers she has left, we subtract: \n",
      " \n",
      "\\[ \n",
      "393 - 89 = 304 \n",
      "\\] \n",
      " \n",
      "So, Aisha has 304 stickers now.\n"
     ]
    }
   ],
   "source": [
    "print(process_latex_string(all_outputs_dct['P3WA']['solution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0031379-fd59-4608-a224-01e60c027524",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./solution/correct solution/td_questions/{model_path}_{file_id}_solution.json', 'w') as file:\n",
    "    json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d1a17-e104-4195-9f82-b266847ee0f0",
   "metadata": {},
   "source": [
    "## Amazon Bedrock loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128df71-bcc1-4c03-99ac-2a5540cca3c8",
   "metadata": {},
   "source": [
    "# Main LLM evaluating given solutions loop\n",
    "\n",
    "This code let ```deepseek/deepseek-r1``` model evaluate the performance of other model.\n",
    "\n",
    "In the future, this code will expand to let each model evaluate one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098b0d8-6ce9-45bd-8a83-8407624e7191",
   "metadata": {},
   "source": [
    "## Helper function to get evaluation summary\n",
    "\n",
    "The summary score will be calculated as follow:\n",
    "```given_info_score, kc_score, final_ans_score, grammar_score``` is calculated by the ```total number of YES``` divided by ```total number of questions```\n",
    "\n",
    "```understand_score, fluency_score``` is calculated by taking the average score given to all the questions\n",
    "\n",
    "All of the variables are then scaled to have the max score of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca101c6c-89e7-47de-9d7e-79c3958002b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation score for models over multiple question\n",
    "def get_evaluation_score(evaluation):\n",
    "    final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score = 0,0,0,0,0,0\n",
    "    evaluated_evaluations = 0\n",
    "    error_flag = False\n",
    "    for item in evaluation.keys():\n",
    "        try:\n",
    "            given_info_correct, kc_correct, final_ans_correct, understand, fluency, grammar_correct = (evaluation[item]['given_info_correct'], evaluation[item]['kc_correct'],\n",
    "                                                    evaluation[item]['final_ans_correct'], evaluation[item]['understand'], evaluation[item]['fluency'],\n",
    "                                                    evaluation[item]['grammar_correct'])\n",
    "            evaluated_evaluations += 1\n",
    "            if evaluation[item]['given_info_correct'] == 'y':\n",
    "                given_info_score += 1\n",
    "            if evaluation[item]['kc_correct'] == 'y':\n",
    "                kc_score += 1\n",
    "            if evaluation[item]['final_ans_correct'] == 'y':\n",
    "                final_ans_score += 1\n",
    "            understand_score += int(evaluation[item]['understand'])\n",
    "            fluency_score += int(evaluation[item]['fluency'])\n",
    "            if evaluation[item]['grammar_correct'] == 'y':\n",
    "                grammar_score += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get evaluation score for item {item}. Error message: {e}\")\n",
    "            print(f\"Item content: {evaluation[item]}\")\n",
    "            error_flag = True\n",
    "    if evaluated_evaluations == 0:\n",
    "        return None\n",
    "    final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score = [x*100/evaluated_evaluations for x in\n",
    "            (final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score)]\n",
    "    understand_score /= 5\n",
    "    fluency_score /= 5\n",
    "    return given_info_score, kc_score, final_ans_score, understand_score, fluency_score, grammar_score, error_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db521e-8ba1-4284-8216-9a59bfa7ae1a",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7429e197-51e2-48af-83d2-a307d6a86bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=TOKEN,\n",
    ")\n",
    "\n",
    "model_evaluator = \"deepseek/deepseek-r1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da33de02-1699-40fa-ae49-050db8dd957d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_10588\\2641241434.py:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently evaluating model:  openai/gpt-4.1\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': '5', 'grammar_correct': 'y', 'fluency': '5'} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_10588\\2641241434.py:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     25\u001b[39m     prompt = {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m You are an expert educator and your role is to grade the student\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms answer. \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     55\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     response = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     message = response.choices[\u001b[32m0\u001b[39m].message\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m message.content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:928\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    927\u001b[39m     response.close()\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:922\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:881\u001b[39m, in \u001b[36mResponse.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03mRead and return the response content.\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_content\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_evaluations_summary = {}\n",
    "\n",
    "for model in models[:1]:\n",
    "    print(\"Currently evaluating model: \", model)\n",
    "\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./solution/{model_path}_solution.json', 'r') as file:\n",
    "        all_outputs = json.load(file)\n",
    "\n",
    "    evaluation = {}\n",
    "    errors = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for i in range(1, len(dataset.keys())+1):\n",
    "        info = {\n",
    "            \"question\" : dataset[str(i)],\n",
    "            \"answer\" : all_outputs.get(str(i), None),\n",
    "            \"kc\": None\n",
    "        }\n",
    "\n",
    "        if info[\"answer\"] is None:\n",
    "            continue\n",
    "    \n",
    "        prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" You are an expert educator and your role is to grade the student's answer. \n",
    "            You are given the question, the student's answer, and you need to grade them based on these rubrics:\n",
    "    \n",
    "            -Task-oriented dimensions:\n",
    "                Given information is correct: y/n\n",
    "                Knowledge component (KC) applied is correct: y/n\n",
    "                Final answer is correct: y/n\n",
    "                \n",
    "    \n",
    "            -Linguistic dimension:\n",
    "                Understandable (1 to 5)\n",
    "                Grammatically correct (y/n)\n",
    "                Fluency, clarity, conciseness (1 to 5)\n",
    "    \n",
    "            The question is: {info[\"question\"]}\n",
    "    \n",
    "            The answer is: {info[\"answer\"]}\n",
    "    \n",
    "            Your final response should be in the json format as below, without any other comments, use strings 'y' or 'n' for y/n:\n",
    "            {{'given_info_correct' : ,\n",
    "                'kc_correct' : ,\n",
    "                'final_ans_correct' : ,\n",
    "                'understand' : ,\n",
    "                'grammar_correct' : ,\n",
    "                'fluency' :\n",
    "            }}\n",
    "    \n",
    "    \"\"\"\n",
    "        }\n",
    "        \n",
    "        response = evaluator.chat.completions.create(\n",
    "                model=model_evaluator,\n",
    "                messages=[prompt],\n",
    "                temperature=0.5\n",
    "            )\n",
    "    \n",
    "        message = response.choices[0].message\n",
    "        if message is None or message.content is None:\n",
    "            print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "            continue\n",
    "    \n",
    "        response_text = message.content.strip()\n",
    "\n",
    "        response_text_extracted = None\n",
    "        match = re.findall(r'\\{[^}]*\\}', response_text)\n",
    "        if match:\n",
    "            response_text_extracted = match[0]\n",
    "        else:\n",
    "            print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "            print(response_text)\n",
    "    \n",
    "        try:\n",
    "            response_dct = ast.literal_eval(response_text_extracted)\n",
    "            evaluation[i] = response_dct\n",
    "        except Exception as e:\n",
    "            errors[i] = response_text\n",
    "            print(\"Found error at\", i, \"error: \", e)\n",
    "            \n",
    "        print(\"Question \", i, \" loaded successfully\")\n",
    "    \n",
    "        if i == 1:\n",
    "            print(evaluation[i], type(evaluation[i]))\n",
    "\n",
    "    model_evaluator_path = model_evaluator.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./evaluation/correct solution/{model_evaluator_path}_evaluating_{model_path}.json', 'w') as file:\n",
    "        json.dump(evaluation, file, indent=4)\n",
    "\n",
    "    if errors:\n",
    "        with open(f'./error/{model_evaluator_path}_evaluating_{model_path}.json', 'w') as file:\n",
    "            json.dump(errors, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddaaa1-1598-4465-b1d3-5d953dac7ad6",
   "metadata": {},
   "source": [
    "## Model evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba2066d-b5bd-49e1-9ce9-5885fadb8781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek_deepseek-r1_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_microsoft_phi-4-reasoning-plus.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_openai_gpt-4.1.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_qwen_qwen3-14b.json analyzed successfully\n",
      "Failed to get evaluation score for item 8. Error message: 'kc_correct'\n",
      "Item content: {'given_info_correct': 'y', 'kc_c': 'y', 'final_ans_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5}\n",
      "WARNING: deepseek_deepseek-r1_evaluating_qwen_qwen3-235b-a22b.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "Failed to get evaluation score for item 5. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 17. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 21. Error message: invalid literal for int() with base 10: '3.2'\n",
      "Item content: {'given_info_correct': 'yes', 'kc_correct': 'yes', 'final_ans_correct': 'yes', 'understand': '3.2', 'grammar_correct': 'yes', 'fluency': '3.2'}\n",
      "Failed to get evaluation score for item 22. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 23. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "WARNING: microsoft_phi-3.5-mini-128k-instruct_evaluating_microsoft_phi-4-reasoning-plus.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "Failed to get evaluation score for item 6. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'n', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "WARNING: microsoft_phi-3.5-mini-128k-instruct_evaluating_qwen_qwen3-14b.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_qwen_qwen3-235b-a22b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_microsoft_phi-4-reasoning-plus.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_qwen_qwen3-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_qwen_qwen3-235b-a22b.json analyzed successfully\n",
      "openai_gpt-4.1_evaluating_openai_gpt-4.1.json analyzed successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "all_evaluations_summary = {}\n",
    "\n",
    "folder_path = Path(\"./evaluation/correct solution\")\n",
    "\n",
    "for json_file in folder_path.glob(\"*.json\"):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            evaluation = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse {json_file.name}: {e}\")\n",
    "        \n",
    "    evaluation_name = json_file.name\n",
    "\n",
    "    evaluation_summary = get_evaluation_score(evaluation)\n",
    "\n",
    "\n",
    "    given_info_score, kc_score, final_ans_score, understand_score, fluency_score, grammar_score, error_flag = evaluation_summary\n",
    "\n",
    "    if error_flag:\n",
    "        print(f\"WARNING: {evaluation_name} contains erronous evaluation\")\n",
    "    else:\n",
    "        print(f\"{evaluation_name} analyzed successfully\")\n",
    "    \n",
    "    all_evaluations_summary[evaluation_name] = {\"given_info_score\" : given_info_score, \"kc_score\":kc_score, \"final_ans_score\":final_ans_score,\n",
    "                                                \"understand_score\" : understand_score, \"fluency_score\":fluency_score, \"grammar_score\":grammar_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "346440df-9e23-4005-83df-4d3aeee719f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./final_summary.json\", \"w\") as file:\n",
    "    json.dump(all_evaluations_summary, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A*Star",
   "language": "python",
   "name": "astar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
