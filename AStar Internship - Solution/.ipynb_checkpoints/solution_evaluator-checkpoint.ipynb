{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b126427-409f-41ce-b853-c7514387b1cc",
   "metadata": {},
   "source": [
    "# Importing libraries, datasets, and correct solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d7ac26-0876-40a3-a168-791c861166fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "dict_keys(['P1N100', 'P1AddSub', 'P1MulDiv', 'P1Money', 'P2N1000', 'P2FracW', 'P2FracAS', 'P3N10k', 'P3FracEq', 'P4N100k', 'P4FctorM', 'P44Op', 'P4FracMI', 'P4FracSet', 'P4Deci3d', 'P4DeciAS', 'P4DeciMD', 'P5N1m', 'P5FracDv', 'P5Frac4Op', 'P5Deci4Op', 'P5perctg', 'P5rate', 'P6ratio', 'P6algebr', 'O1NumOps', 'O1RioPro', 'O1RatSpd', 'O1AgbrEF', 'O1EqIneq', 'O2Prob', 'O3NumOps'])\n"
     ]
    }
   ],
   "source": [
    "#Import important libraries\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "file_id = \"2025-06-09_14_16_30\"\n",
    "\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"AStarPrivate\")\n",
    "if TOKEN is None:\n",
    "    raise RuntimeError(\"Token parsing failed\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=TOKEN,\n",
    ")\n",
    "\n",
    "models = [\"openai/gpt-4.1\", \"meta-llama/llama-4-scout\", \"deepseek/deepseek-r1\", \n",
    "          \"microsoft/phi-4\", \"qwen/qwen3-14b\", \"anthropic/claude-3.7-sonnet\"]\n",
    "models_free = [\"microsoft/phi-4-reasoning-plus:free\", \"deepseek/deepseek-r1-distill-qwen-32b:free\", \"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "              \"qwen/qwen3-32b:free\"]\n",
    "model_default = \"deepseek/deepseek-r1-distill-qwen-32b:free\"\n",
    "model = None\n",
    "\n",
    "with open(f\"./dataset/{file_id}_generated_questions.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "print(len(dataset.keys()))\n",
    "print(dataset.keys())\n",
    "\n",
    "#For printing aesthetics\n",
    "white_space = \" \" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73825e34-2396-44a3-a889-c8f79077828e",
   "metadata": {},
   "source": [
    "# Step 1: Correct solution generation\n",
    "Refer to 'solution_generator' file for the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18d9e7-19d0-476d-8a40-1d4e5b93b4d9",
   "metadata": {},
   "source": [
    "# Step 2: Evaluate the given solution based on own correct solutions\n",
    "\n",
    "Evaluate the given solution, using the generated solution above. The generated solution should come from the same model that is evaluating, to maintain fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19612f32-2e4c-4c0b-bf29-34e379e118ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0571dd35-58f6-4916-a41c-e5b66649e3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you sure you want to use chatgpt to generate 240 responses? (Empty answer for no) 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current evaluator model: openai/gpt-4.1 (1/6)\n",
      "Currently evaluating model: openai/gpt-4.1 (1/6)\n",
      "Evaluating question P6ratio (1/15)                                                  \n",
      "Evaluating question O3NumOps (2/15)                                                                              \n",
      "Evaluating question P5Deci4Op (3/15)                                                                             \n",
      "Evaluating question P5rate (4/15)                                                                                \n",
      "Evaluating question P2FracW (5/15)                                                                               \n",
      "Evaluating question P4FctorM (6/15)                                                                              \n",
      "Evaluating question P3N10k (7/15)                                                                                \n",
      "Evaluating question P5FracDv (8/15)                                                                              \n",
      "Evaluating question O1AgbrEF (9/15)                                                                              \n",
      "Evaluating question O1NumOps (10/15)                                                                             \n",
      "Evaluating question P4Deci3d (11/15)                                                                             \n",
      "Evaluating question P4N100k (12/15)                                                                              \n",
      "Evaluating question O1EqIneq (13/15)                                                                             \n",
      "Evaluating question P1Money (14/15)                                                                              \n",
      "Evaluating question P4FracMI (15/15)                                                                             \n",
      "Evaluating solution Incorrect Copying of Numbers (9/9)                                                           "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block is used to evaluate the given solution\n",
    "\n",
    "Input:\n",
    "dataset (dct) : { (question ID) : {'ID' : question ID,\n",
    "                                   'Grade' : the grade the question is in,\n",
    "                                   'KC' : knowledge component of the question,\n",
    "                                   'Question' : question content\n",
    "                                   } : Question dataset\n",
    "given_solution (for custom mode) (dct): { (question ID) : {(solution ID) : {'question' : question content\n",
    "                                                                            'solution' : {'step (number)' : step content}\n",
    "                                                                           }\n",
    "                                                          }\n",
    "                                        }: Given solution to the question\n",
    "model_solution (dct): { (question ID) : {'question' : question content,\n",
    "                                         'kc' : knowledge component of this question,\n",
    "                                         'solution' : {'step (number)' : step content}\n",
    "                                        }\n",
    "                      } : Model solution that the model should base on to evaluate the given solution\n",
    "## Note that all question ID must match with each other\n",
    "models : List of large language models to be used\n",
    "system_prompt_fpath : System prompt text file path\n",
    "user_prompt_fpath : User prompt text file path\n",
    "solution_type_list : Types of solution to be evaluated, correct means the evaluator will evaluate correct solutions, incorrect means the evaluator will\n",
    "evaluate incorrect solutions. This is only used to get the dataset from the correct file and return the evaluation in the right structure, thus would not\n",
    "affect the performance of the LLM.\n",
    "solution_type : Type of solution to be evaluated. Choose from solution_type_list.\n",
    "\"\"\"\n",
    "# dataset, models should be loaded before step 1\n",
    "solution_type_list = ['correct','wrong','custom']\n",
    "solution_type = solution_type_list[1]\n",
    "given_solution = {}\n",
    "\n",
    "with open('./solution/wrong solution/step 2/combined_error_types.json','r') as file:\n",
    "    error_types = json.load(file)\n",
    "\n",
    "evaluating_model_idx = 0\n",
    "for model_evaluator in models[:1]:\n",
    "    if model_evaluator == models[0]:\n",
    "        confirmation = input(\"Are you sure you want to use chatgpt to generate 240 responses? (Empty answer for no)\")\n",
    "        if not confirmation:\n",
    "            sys.exit(0)\n",
    "    evaluating_model_idx = 1\n",
    "    print(f\"Current evaluator model: {model_evaluator} ({evaluating_model_idx}/{len(models)})\")\n",
    "\n",
    "    #Initialize output\n",
    "    evaluation_dct = {}\n",
    "    evaluation_text = {}\n",
    "    errors = {}\n",
    "\n",
    "    #Replace / to _ so that it would not interfere with file path:\n",
    "    model_evaluator_path = model_evaluator.replace(\"/\",\"_\")\n",
    "    system_prompt_fpath = f\"./prompt/evaluation/{model_evaluator_path}_system_prompt.txt\" \n",
    "    user_prompt_fpath = f\"./prompt/evaluation/{model_evaluator_path}_user_prompt.txt\"\n",
    "    generated_evaluation_fpath = f'./evaluation/{solution_type} solution/dataset_{file_id}/{model_evaluator_path}_evaluating.json'\n",
    "    with open(f\"./solution/correct solution/dataset_{file_id}/{model_evaluator_path}_solution.json\") as file:\n",
    "        model_solution = json.load(file)\n",
    "        \n",
    "    #Load prompt for solution evaluation\n",
    "    try:\n",
    "        with open(system_prompt_fpath,\"r\") as file:\n",
    "            system_prompt = file.read()\n",
    "    except Exception as e:\n",
    "        system_prompt = \"\"\n",
    "    try:\n",
    "        with open(user_prompt_fpath,\"r\") as file:\n",
    "            user_prompt = file.read()\n",
    "    except Exception as e:\n",
    "        user_prompt = \"\"\n",
    "    \n",
    "    evaluated_model_idx = 0\n",
    "    for model in models[:1]:\n",
    "        evaluated_model_idx += 1\n",
    "        print(f\"Currently evaluating model: {model} ({evaluated_model_idx}/{len(models)})\", flush = True)\n",
    "        model_path = model.replace(\"/\",\"_\")\n",
    "\n",
    "        #Load solutions to evaluate\n",
    "        if solution_type == 'correct':\n",
    "            correct_solutions_repo = f\"./solution/correct solution/dataset_{file_id}/{model_path}_solution.json\"\n",
    "            with open(correct_solutions_repo) as file:\n",
    "                loaded_solution = json.load(file)\n",
    "            for question_id in loaded_solution.keys():\n",
    "                given_solution[question_id] = {}\n",
    "                given_solution[question_id][\"Correct Solution\"] = loaded_solution[question_id]\n",
    "        elif solution_type == 'wrong':\n",
    "            wrong_solutions_repo = f\"./solution/wrong solution/step 3/dataset_{file_id}/temporary_files/{model_path}_wrong_solution.json\"\n",
    "            with open(wrong_solutions_repo) as file:\n",
    "                given_solution = json.load(file)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        evaluation_dct[model] = {}\n",
    "        evaluation_text[model] = {}\n",
    "        errors[model] = {}\n",
    "        \n",
    "        # Assuming primary_df is defined elsewhere\n",
    "        question_idx = 0\n",
    "        for questionID in list(given_solution.keys()):\n",
    "            question_idx += 1\n",
    "            evaluation_dct[model][questionID] = {}\n",
    "            evaluation_text[model][questionID] = {}\n",
    "            print(f\"\\rEvaluating question {questionID} ({question_idx}/{len(given_solution.keys())}){white_space}\")\n",
    "\n",
    "            solution_idx = 0\n",
    "            for solutionID in given_solution[questionID].keys():\n",
    "                solution_idx += 1\n",
    "                print(f\"\\rEvaluating solution {solutionID} ({solution_idx}/{len(given_solution[questionID].keys())}){white_space}\", end = '', flush=True)\n",
    "                info = {\n",
    "                    \"question\" : given_solution[questionID][solutionID][\"question\"],\n",
    "                    \"model solution\" : model_solution.get(questionID, {}).get(\"solution\", None),\n",
    "                    \"given solution\" : given_solution[questionID][solutionID][\"solution\"] if solution_type == 'correct' or solution_type == 'custom'\n",
    "                                       else given_solution[questionID][solutionID][\"wrong_solution\"],\n",
    "                    \"kc\": dataset[questionID][\"Grade\"] + ' ' + dataset[questionID][\"KC\"],\n",
    "                    \"error_types\" : error_types[questionID][\"type_of_error\"],\n",
    "                    \"error_explanation\" : error_types[questionID][\"error_explanation\"]\n",
    "                }\n",
    "            \n",
    "                prompt = [{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": system_prompt.format(),\n",
    "                          },\n",
    "                          {\n",
    "                              \"role\" : \"user\",\n",
    "                              \"content\" : user_prompt.format(\n",
    "                                  question=info[\"question\"],\n",
    "                                  model_solution=info[\"model solution\"],\n",
    "                                  given_solution=info[\"given solution\"],\n",
    "                                  kc=info[\"kc\"],\n",
    "                                  type_of_error = info[\"error_types\"],\n",
    "                                  error_explanation = info[\"error_explanation\"]\n",
    "                              )\n",
    "                          }]\n",
    "                \n",
    "                #Get response from API call\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model_evaluator,\n",
    "                        messages=prompt,\n",
    "                        temperature=0.9\n",
    "                    )\n",
    "                message = response.choices[0].message\n",
    "                if message is None or message.content is None:\n",
    "                    print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "                    continue    \n",
    "                response_text = message.content.strip()\n",
    "                response_text_extracted = None\n",
    "    \n",
    "                #Extract and convert to dictionary\n",
    "                match = re.findall(r'```(.*?)```', response_text, re.DOTALL)\n",
    "                if match:\n",
    "                    response_text_extracted = match[-1]\n",
    "                    match = re.findall(r'\\{.*\\}', response_text_extracted, re.DOTALL)\n",
    "                else:\n",
    "                    match = re.findall(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                if match:\n",
    "                    response_text_extracted = match[0]\n",
    "                else:\n",
    "                    print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "                    print(response_text)   \n",
    "                try:\n",
    "                    response_dct = ast.literal_eval(response_text_extracted)\n",
    "                    evaluation_dct[model][questionID][solutionID] = response_dct\n",
    "                except Exception as e:\n",
    "                    evaluation_text[model][questionID][solutionID] = response_text\n",
    "                    print(\"Found error at\", questionID, model, solutionID, \"error: \", e)\n",
    "                    print(response_text)\n",
    "    \n",
    "    #Save to file\n",
    "    os.makedirs(os.path.dirname(generated_evaluation_fpath), exist_ok=True)\n",
    "    with open(generated_evaluation_fpath, 'w') as file:\n",
    "        json.dump(evaluation_dct, file, indent=4)\n",
    "\n",
    "    os.makedirs(os.path.dirname(f'./error/raw_responses/{model_evaluator_path}_evaluating.json'), exist_ok=True)\n",
    "    with open(f'./error/raw_responses/{model_evaluator_path}_evaluating.json','w') as file:\n",
    "        json.dump(evaluation_text,file,indent=4)\n",
    "\n",
    "    if errors:\n",
    "        with open(f'./error/{model_evaluator_path}_evaluating_{model_path}.json', 'w') as file:\n",
    "            json.dump(errors, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7c89834-1f50-4313-9277-51f1e544e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(generated_evaluation_fpath), exist_ok=True)\n",
    "with open(generated_evaluation_fpath, 'w') as file:\n",
    "    json.dump(evaluation_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73ff27a-aef8-4494-bf93-30750903e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(generated_evaluation_fpath), exist_ok=True)\n",
    "with open(generated_evaluation_fpath, 'w') as file:\n",
    "    json.dump(evaluation_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6187d7a6-d385-4669-91fd-5f9fa2ff08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"label\": \"sk-or-v1-8ce...e3b\",\n",
      "    \"limit\": null,\n",
      "    \"usage\": 21.1609805,\n",
      "    \"is_provisioning_key\": false,\n",
      "    \"limit_remaining\": null,\n",
      "    \"is_free_tier\": false,\n",
      "    \"rate_limit\": {\n",
      "      \"requests\": 450,\n",
      "      \"interval\": \"10s\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(\n",
    "  url=\"https://openrouter.ai/api/v1/auth/key\",\n",
    "  headers={\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\"\n",
    "  }\n",
    ")\n",
    "\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f59b48-e016-43a0-9250-49c6096353b9",
   "metadata": {},
   "source": [
    "### 2'. Reinserting erroneous responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "723cbeac-953d-4733-bd4e-b71b25e2389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['anthropic/claude-3.7-sonnet'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_3120\\706155088.py:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  fixed_response = \"\"\"{\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_3120\\706155088.py:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  fixed_response = \"\"\"{\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'openai/gpt-4.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m\n\u001b[0;32m     16\u001b[0m     response_dct \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(fixed_response)\n\u001b[1;32m---> 17\u001b[0m     evaluation_dct[model][questionID][solutionID] \u001b[38;5;241m=\u001b[39m response_dct\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'openai/gpt-4.1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     evaluation_dct[model][questionID][solutionID] \u001b[38;5;241m=\u001b[39m response_dct\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 19\u001b[0m     evaluation_text[model][questionID][solutionID] \u001b[38;5;241m=\u001b[39m response_text\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound error at\u001b[39m\u001b[38;5;124m\"\u001b[39m, questionID, model, solutionID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror: \u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response_text)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'openai/gpt-4.1'"
     ]
    }
   ],
   "source": [
    "model_evaluator_path = models[4].replace(\"/\",\"_\")\n",
    "with open(f'./evaluation/{solution_type} solution/dataset_{file_id}/{model_evaluator_path}_evaluating.json', 'r') as file:\n",
    "    evaluation_dct = json.load(file)\n",
    "    print(evaluation_dct.keys())\n",
    "model = models[0]\n",
    "questionID = 'O1EqIneq'\n",
    "solutionID = 'Invalid Assumption'\n",
    "fixed_response = \"\"\"{\n",
    "\"question\" : \"A mobile phone usually costs \\$ x \\$ dollars. During a sale, its price is reduced by 50 dollars and the new price is 320 dollars. Write down an equation to represent this situation and find the value of \\$ x \\$.\",\n",
    "\"correct\" : \"no\",\n",
    "\"wrong_solution\" : {\"Step 2\": \"Assume there is an additional service charge of 30 dollars added during the sale.\", \"Step 3\": \"So, the new price after reduction and adding the service charge is \\$ x - 50 + 30 \\$ dollars.\"},\n",
    "\"type_of_error\" : {\"Step 2\": \"Invalid Assumption\"},\n",
    "\"error_explanation\" : {\"Step 2\": \"The student made an invalid assumption by introducing an additional service charge of 30 dollars, which is not mentioned in the question. This extra information is not part of the original problem and affects the solution unnecessarily.\"},\n",
    "\"correct_solution\" : {\"Step 1\": \"The original price of the mobile phone is \\$ x \\$. During the sale, the price is reduced by 50 dollars, so the new price is \\$ x - 50 \\$ dollars.\", \"Step 2\": \"We are told that the new price is 320 dollars. Therefore, the equation representing this situation is \\$ x - 50 = 320 \\$.\", \"Step 3\": \"To find the value of \\$ x \\$, we solve the equation \\$ x - 50 = 320 \\$. Adding 50 to both sides of the equation gives \\$ x = 320 + 50 \\$.\", \"Step 4\": \"Simplifying the right side of the equation, we get \\$ x = 370 \\$.\"}}\"\"\"\n",
    "try:\n",
    "    response_dct = ast.literal_eval(fixed_response)\n",
    "    evaluation_dct[model][questionID][solutionID] = response_dct\n",
    "except Exception as e:\n",
    "    evaluation_text[model][questionID][solutionID] = response_text\n",
    "    print(\"Found error at\", questionID, model, solutionID, \"error: \", e)\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c36d4-4486-46c9-92bc-a4e6d91c3a15",
   "metadata": {},
   "source": [
    "## 3. High quality dataset extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f184d0-4039-4876-8dc3-62dd2898a855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
