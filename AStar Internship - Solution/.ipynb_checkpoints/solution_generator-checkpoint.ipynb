{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9c4948-c928-4286-95bb-0894c64b88c9",
   "metadata": {},
   "source": [
    "### Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11929bb-0d51-4909-95d1-c776b7a059b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important libraries\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163626b9-7aee-48ae-abfd-a668e51c8583",
   "metadata": {},
   "source": [
    "### Import original PSLE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bae520b-6d8f-4e17-8d28-032121eb638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/generated_questions.json\", \"r\") as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ef47ed-d958-4d2f-8c42-25d0835a2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict_to_depth(d, max_depth, separator='_'):\n",
    "    def _flatten(current, parent_key='', depth=0):  # Start from depth = 0\n",
    "        items = {}\n",
    "        for k, v in current.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict) and depth < max_depth:\n",
    "                items.update(_flatten(v, new_key, depth + 1))\n",
    "            else:\n",
    "                items[new_key] = v\n",
    "        return items\n",
    "\n",
    "    return _flatten(d)\n",
    "\n",
    "def get_max_depth(d):\n",
    "    if not isinstance(d, dict) or not d:\n",
    "        return 0\n",
    "    depth_list = [get_max_depth(v) for v in d.values() if isinstance(v, dict)]\n",
    "    if not depth_list:\n",
    "        return 1\n",
    "    if depth_list is None:\n",
    "        return 0\n",
    "    return 1 + max(depth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eeb265b-1a55-440c-9c77-be0b1c9c5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_flat = flatten_dict_to_depth(dataset, get_max_depth(dataset) - 1, separator='_')\n",
    "dataset_gpt41 = dataset['gpt41']\n",
    "result = kc_df.groupby(kc_df.columns[4])[kc_df.columns[5]].apply(list).to_dict()\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f8f1d-ff6d-40a1-a876-fa08cb41f9ca",
   "metadata": {},
   "source": [
    "# Initialize keys and token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5718f-fc27-4e60-a5a9-e14f10bbc01b",
   "metadata": {},
   "source": [
    "## OpenRouter with OpenAI wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cd32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"AStarPrivate\")\n",
    "if TOKEN is None:\n",
    "    raise RuntimeError(\"Token parsing failed\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=TOKEN,\n",
    ")\n",
    "\n",
    "models = [\"openai/gpt-4.1\", \"anthropic/claude-3.7-sonnet\", \"deepseek/deepseek-r1\", \n",
    "          \"microsoft/phi-4\", \"qwen/qwen3-14b\", \n",
    "          \"meta-llama/llama-4-scout\"]\n",
    "models_free = [\"microsoft/phi-4-reasoning-plus:free\", \"deepseek/deepseek-r1-distill-qwen-32b:free\", \"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "              \"qwen/qwen3-32b:free\"]\n",
    "model_default = \"deepseek/deepseek-r1-distill-qwen-32b:free\"\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c1594-aaff-48c9-ac7c-21fe7fd704d2",
   "metadata": {},
   "source": [
    "# Main LLM solving PSLE questions loop (correct solutions)\n",
    "\n",
    "This code let each of the LLM solve PSLE questions and store those answers in the ```solutions``` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17205d15-1572-40d4-92a7-f69c4de2f23c",
   "metadata": {},
   "source": [
    "## OpenRouter Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92d88288-4793-42ae-9ab4-b8c65515ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working with model:  openai/gpt-4.1\n",
      "Knowledge component O1NR (1/23) loaded successfully\n",
      "Knowledge component O2NA (2/23) loaded successfully\n",
      "Knowledge component O2NE (3/23) loaded successfully\n",
      "Knowledge component O2NR (4/23) loaded successfully\n",
      "Knowledge component O2SP (5/23) loaded successfully\n",
      "Knowledge component P2FF (6/23) loaded successfully\n",
      "Knowledge component P3FE (7/23) loaded successfully\n",
      "Knowledge component P3MM (8/23) loaded successfully\n",
      "Knowledge component P3WA (9/23) loaded successfully\n",
      "Knowledge component P3WM (10/23) loaded successfully\n",
      "Knowledge component P4DA (11/23) loaded successfully\n",
      "Knowledge component P4DD (12/23) loaded successfully\n",
      "Knowledge component P4DM (13/23) loaded successfully\n",
      "Knowledge component P4FA (14/23) loaded successfully\n",
      "Knowledge component P4FF (15/23) loaded successfully\n",
      "Knowledge component P4FM (16/23) loaded successfully\n",
      "Knowledge component P4WF (17/23) loaded successfully\n",
      "Knowledge component P4WN (18/23) loaded successfully\n",
      "Knowledge component P5FF (19/23) loaded successfully\n",
      "Knowledge component P5PP (20/23) loaded successfully\n",
      "Knowledge component P5RR (21/23) loaded successfully\n",
      "Knowledge component P6AA (22/23) loaded successfully\n",
      "Knowledge component P6RR (23/23) loaded successfully\n"
     ]
    }
   ],
   "source": [
    "for model in models[0:1]:\n",
    "    kc_index = 0\n",
    "    dataset_length = len(dataset.keys())\n",
    "\n",
    "    print(\"Currently working with model: \", model)\n",
    "\n",
    "    all_outputs_dct = {}\n",
    "    all_outputs_text = {}\n",
    "    errors = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for kcID in dataset.keys():\n",
    "        kc_index += 1\n",
    "        info = {\n",
    "            \"question\" : dataset[kcID][\"Question\"],\n",
    "            \"answer\" : None,\n",
    "            \"kc\": None\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(f\"./prompt/correct solution/{model_path}_system_prompt.txt\",\"r\") as file:\n",
    "                system_prompt = file.read()\n",
    "        except Exception as e:\n",
    "            system_prompt = \"\"\n",
    "        try:\n",
    "            with open(f\"./prompt/correct solution/{model_path}_user_prompt.txt\",\"r\") as file:\n",
    "                user_prompt = file.read()\n",
    "        except Exception as e:\n",
    "            user_prompt = \"\"\n",
    "    \n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : user_prompt.format(question = info[\"question\"])\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "        try:\n",
    "            if model is None:\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model_default,\n",
    "                        messages=prompt,\n",
    "                        temperature=0.5\n",
    "                    )\n",
    "            elif model == \"deepseek/deepseek-r1-distill-qwen-14b\":\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=prompt,\n",
    "                        temperature=0.5,\n",
    "                        max_tokens=30000 \n",
    "                    )\n",
    "            else:\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=prompt,\n",
    "                        temperature=0.5\n",
    "                    )\n",
    "    \n",
    "            message = response.choices[0].message\n",
    "            if message is None or message.content is None:\n",
    "                print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "                continue\n",
    "    \n",
    "            response_text = message.content.strip()\n",
    "\n",
    "            response_text_extracted = None\n",
    "            match = re.findall(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            if match:\n",
    "                response_text_extracted = match[0]\n",
    "            else:\n",
    "                print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "                print(response_text)\n",
    "        \n",
    "            try:\n",
    "                response_dct = ast.literal_eval(response_text_extracted)\n",
    "                all_outputs_dct[kcID] = response_dct\n",
    "            except Exception as e:\n",
    "                errors[kcID] = response_text\n",
    "                print(\"Found error at\", kcID, \"error: \", e)\n",
    "                print(response_text)\n",
    "    \n",
    "            all_outputs_text[kcID] = response_text\n",
    "\n",
    "    \n",
    "            print(f\"Knowledge component {kcID} ({kc_index}/{dataset_length}) loaded successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call for KC: {kcID} - {e}\")\n",
    "\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "    \n",
    "    if model is None:\n",
    "        with open(f'./solution/correct solution/td_questions/{model_default}_solution.json', 'w') as file:\n",
    "            json.dump(all_outputs_dct, file, indent=4)\n",
    "    else:\n",
    "        with open(f'./solution/correct solution/td_questions/{model_path}_solution.json', 'w') as file:\n",
    "            json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "391c7adb-19bc-40b7-be1e-02754dabc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./solution/correct solution/td_questions/{model_path}_solution.json', 'w') as file:\n",
    "    json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "903bd93a-d223-45ee-abd9-acbb8443e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing.txt\",\"w\") as file:\n",
    "    file.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a9d3cdd-7fbb-4e92-bef7-e9aa84212b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_latex_string(s):\n",
    "    s = s.replace('\\\\\\\\', '\\\\')   # Normalize double backslashes\n",
    "    s = s.replace('\\\\n', r'\\\\')   # Replace \\n with LaTeX newline command\n",
    "    return s\n",
    "\n",
    "    \n",
    "    return s\n",
    "\n",
    "a = process_latex_string(a)\n",
    "\n",
    "with open(\"./testing.txt\",\"w\") as file:\n",
    "    file.write(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d1a17-e104-4195-9f82-b266847ee0f0",
   "metadata": {},
   "source": [
    "## Amazon Bedrock loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128df71-bcc1-4c03-99ac-2a5540cca3c8",
   "metadata": {},
   "source": [
    "# Main LLM evaluating given solutions loop\n",
    "\n",
    "This code let ```deepseek/deepseek-r1``` model evaluate the performance of other model.\n",
    "\n",
    "In the future, this code will expand to let each model evaluate one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098b0d8-6ce9-45bd-8a83-8407624e7191",
   "metadata": {},
   "source": [
    "## Helper function to get evaluation summary\n",
    "\n",
    "The summary score will be calculated as follow:\n",
    "```given_info_score, kc_score, final_ans_score, grammar_score``` is calculated by the ```total number of YES``` divided by ```total number of questions```\n",
    "\n",
    "```understand_score, fluency_score``` is calculated by taking the average score given to all the questions\n",
    "\n",
    "All of the variables are then scaled to have the max score of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca101c6c-89e7-47de-9d7e-79c3958002b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation score for models over multiple question\n",
    "def get_evaluation_score(evaluation):\n",
    "    final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score = 0,0,0,0,0,0\n",
    "    evaluated_evaluations = 0\n",
    "    error_flag = False\n",
    "    for item in evaluation.keys():\n",
    "        try:\n",
    "            given_info_correct, kc_correct, final_ans_correct, understand, fluency, grammar_correct = (evaluation[item]['given_info_correct'], evaluation[item]['kc_correct'],\n",
    "                                                    evaluation[item]['final_ans_correct'], evaluation[item]['understand'], evaluation[item]['fluency'],\n",
    "                                                    evaluation[item]['grammar_correct'])\n",
    "            evaluated_evaluations += 1\n",
    "            if evaluation[item]['given_info_correct'] == 'y':\n",
    "                given_info_score += 1\n",
    "            if evaluation[item]['kc_correct'] == 'y':\n",
    "                kc_score += 1\n",
    "            if evaluation[item]['final_ans_correct'] == 'y':\n",
    "                final_ans_score += 1\n",
    "            understand_score += int(evaluation[item]['understand'])\n",
    "            fluency_score += int(evaluation[item]['fluency'])\n",
    "            if evaluation[item]['grammar_correct'] == 'y':\n",
    "                grammar_score += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get evaluation score for item {item}. Error message: {e}\")\n",
    "            print(f\"Item content: {evaluation[item]}\")\n",
    "            error_flag = True\n",
    "    if evaluated_evaluations == 0:\n",
    "        return None\n",
    "    final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score = [x*100/evaluated_evaluations for x in\n",
    "            (final_ans_score, kc_score, given_info_score, understand_score, fluency_score, grammar_score)]\n",
    "    understand_score /= 5\n",
    "    fluency_score /= 5\n",
    "    return given_info_score, kc_score, final_ans_score, understand_score, fluency_score, grammar_score, error_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db521e-8ba1-4284-8216-9a59bfa7ae1a",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7429e197-51e2-48af-83d2-a307d6a86bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=TOKEN,\n",
    ")\n",
    "\n",
    "model_evaluator = \"deepseek/deepseek-r1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da33de02-1699-40fa-ae49-050db8dd957d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_10588\\2641241434.py:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently evaluating model:  openai/gpt-4.1\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': '5', 'grammar_correct': 'y', 'fluency': '5'} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_10588\\2641241434.py:70: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     25\u001b[39m     prompt = {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m You are an expert educator and your role is to grade the student\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms answer. \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     55\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     response = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     message = response.choices[\u001b[32m0\u001b[39m].message\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m message.content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:928\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    927\u001b[39m     response.close()\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:922\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:881\u001b[39m, in \u001b[36mResponse.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03mRead and return the response content.\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_content\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_evaluations_summary = {}\n",
    "\n",
    "for model in models[:1]:\n",
    "    print(\"Currently evaluating model: \", model)\n",
    "\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./solution/{model_path}_solution.json', 'r') as file:\n",
    "        all_outputs = json.load(file)\n",
    "\n",
    "    evaluation = {}\n",
    "    errors = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for i in range(1, len(dataset.keys())+1):\n",
    "        info = {\n",
    "            \"question\" : dataset[str(i)],\n",
    "            \"answer\" : all_outputs.get(str(i), None),\n",
    "            \"kc\": None\n",
    "        }\n",
    "\n",
    "        if info[\"answer\"] is None:\n",
    "            continue\n",
    "    \n",
    "        prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" You are an expert educator and your role is to grade the student's answer. \n",
    "            You are given the question, the student's answer, and you need to grade them based on these rubrics:\n",
    "    \n",
    "            -Task-oriented dimensions:\n",
    "                Given information is correct: y/n\n",
    "                Knowledge component (KC) applied is correct: y/n\n",
    "                Final answer is correct: y/n\n",
    "                \n",
    "    \n",
    "            -Linguistic dimension:\n",
    "                Understandable (1 to 5)\n",
    "                Grammatically correct (y/n)\n",
    "                Fluency, clarity, conciseness (1 to 5)\n",
    "    \n",
    "            The question is: {info[\"question\"]}\n",
    "    \n",
    "            The answer is: {info[\"answer\"]}\n",
    "    \n",
    "            Your final response should be in the json format as below, without any other comments, use strings 'y' or 'n' for y/n:\n",
    "            {{'given_info_correct' : ,\n",
    "                'kc_correct' : ,\n",
    "                'final_ans_correct' : ,\n",
    "                'understand' : ,\n",
    "                'grammar_correct' : ,\n",
    "                'fluency' :\n",
    "            }}\n",
    "    \n",
    "    \"\"\"\n",
    "        }\n",
    "        \n",
    "        response = evaluator.chat.completions.create(\n",
    "                model=model_evaluator,\n",
    "                messages=[prompt],\n",
    "                temperature=0.5\n",
    "            )\n",
    "    \n",
    "        message = response.choices[0].message\n",
    "        if message is None or message.content is None:\n",
    "            print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "            continue\n",
    "    \n",
    "        response_text = message.content.strip()\n",
    "\n",
    "        response_text_extracted = None\n",
    "        match = re.findall(r'\\{[^}]*\\}', response_text)\n",
    "        if match:\n",
    "            response_text_extracted = match[0]\n",
    "        else:\n",
    "            print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "            print(response_text)\n",
    "    \n",
    "        try:\n",
    "            response_dct = ast.literal_eval(response_text_extracted)\n",
    "            evaluation[i] = response_dct\n",
    "        except Exception as e:\n",
    "            errors[i] = response_text\n",
    "            print(\"Found error at\", i, \"error: \", e)\n",
    "            \n",
    "        print(\"Question \", i, \" loaded successfully\")\n",
    "    \n",
    "        if i == 1:\n",
    "            print(evaluation[i], type(evaluation[i]))\n",
    "\n",
    "    model_evaluator_path = model_evaluator.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./evaluation/correct solution/{model_evaluator_path}_evaluating_{model_path}.json', 'w') as file:\n",
    "        json.dump(evaluation, file, indent=4)\n",
    "\n",
    "    if errors:\n",
    "        with open(f'./error/{model_evaluator_path}_evaluating_{model_path}.json', 'w') as file:\n",
    "            json.dump(errors, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddaaa1-1598-4465-b1d3-5d953dac7ad6",
   "metadata": {},
   "source": [
    "## Model evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba2066d-b5bd-49e1-9ce9-5885fadb8781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek_deepseek-r1_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_microsoft_phi-4-reasoning-plus.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_openai_gpt-4.1.json analyzed successfully\n",
      "deepseek_deepseek-r1_evaluating_qwen_qwen3-14b.json analyzed successfully\n",
      "Failed to get evaluation score for item 8. Error message: 'kc_correct'\n",
      "Item content: {'given_info_correct': 'y', 'kc_c': 'y', 'final_ans_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5}\n",
      "WARNING: deepseek_deepseek-r1_evaluating_qwen_qwen3-235b-a22b.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "Failed to get evaluation score for item 5. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 17. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 21. Error message: invalid literal for int() with base 10: '3.2'\n",
      "Item content: {'given_info_correct': 'yes', 'kc_correct': 'yes', 'final_ans_correct': 'yes', 'understand': '3.2', 'grammar_correct': 'yes', 'fluency': '3.2'}\n",
      "Failed to get evaluation score for item 22. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "Failed to get evaluation score for item 23. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'y', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "WARNING: microsoft_phi-3.5-mini-128k-instruct_evaluating_microsoft_phi-4-reasoning-plus.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "Failed to get evaluation score for item 6. Error message: invalid literal for int() with base 10: 'y'\n",
      "Item content: {'given_info_correct': 'y', 'kc_correct': 'y', 'final_ans_correct': 'n', 'understand': 'y', 'grammar_correct': 'y', 'fluency': 'y'}\n",
      "WARNING: microsoft_phi-3.5-mini-128k-instruct_evaluating_qwen_qwen3-14b.json contains erronous evaluation\n",
      "microsoft_phi-3.5-mini-128k-instruct_evaluating_qwen_qwen3-235b-a22b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_deepseek_deepseek-r1.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_meta-llama_llama-4-scout.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_microsoft_phi-4-reasoning-plus.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_qwen_qwen3-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_qwen_qwen3-235b-a22b.json analyzed successfully\n",
      "openai_gpt-4.1_evaluating_openai_gpt-4.1.json analyzed successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "all_evaluations_summary = {}\n",
    "\n",
    "folder_path = Path(\"./evaluation/correct solution\")\n",
    "\n",
    "for json_file in folder_path.glob(\"*.json\"):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            evaluation = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse {json_file.name}: {e}\")\n",
    "        \n",
    "    evaluation_name = json_file.name\n",
    "\n",
    "    evaluation_summary = get_evaluation_score(evaluation)\n",
    "\n",
    "\n",
    "    given_info_score, kc_score, final_ans_score, understand_score, fluency_score, grammar_score, error_flag = evaluation_summary\n",
    "\n",
    "    if error_flag:\n",
    "        print(f\"WARNING: {evaluation_name} contains erronous evaluation\")\n",
    "    else:\n",
    "        print(f\"{evaluation_name} analyzed successfully\")\n",
    "    \n",
    "    all_evaluations_summary[evaluation_name] = {\"given_info_score\" : given_info_score, \"kc_score\":kc_score, \"final_ans_score\":final_ans_score,\n",
    "                                                \"understand_score\" : understand_score, \"fluency_score\":fluency_score, \"grammar_score\":grammar_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "346440df-9e23-4005-83df-4d3aeee719f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./final_summary.json\", \"w\") as file:\n",
    "    json.dump(all_evaluations_summary, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83135773-5d0b-48bf-acb7-0f9f5391910f",
   "metadata": {},
   "source": [
    "# Main LLM solving PSLE questions loop (wrong solutions) (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca2ac22f-e135-4620-808c-9a024266c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_mistakes = [\"Arithmetic Operation Errors\", \"Decimal Place Value Errors\", \"Missing Key Information\", \"Overlooking Units and Conversion\",\n",
    "                    \"Common Formula Mix-ups\", \"Incorrect Formula Application\", \"Geometry and Measurement Mistakes\", \"Pattern Recognition Failures\",\n",
    "                    \"Algebraic Thinking Gaps\", \"Difficulty Translating Words to Mathematical Operations\", \"Multiple-Step Problem Confusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f482b19-92ac-499a-9854-997117764c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working with model:  qwen/qwen3-235b-a22b\n",
      "Question  1  loaded successfully\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently working with model:  qwen/qwen3-14b\n",
      "Question  1  loaded successfully\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently working with model:  meta-llama/llama-4-scout\n",
      "Question  1  loaded successfully\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently working with model:  meta-llama/llama-3.3-70b-instruct\n",
      "Question  1  loaded successfully\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Error during API call for KC: 29. Siti loves kite flying. After school, she spent 45 min at the park to fly her kite and left at 3.35 p.m. At what time did she arrive at the park? - Expecting value: line 2767 column 1 (char 15213)\n",
      "Question  24  loaded successfully\n"
     ]
    }
   ],
   "source": [
    "for model in models[6:]:\n",
    "\n",
    "    print(\"Currently working with model: \", model)\n",
    "\n",
    "    all_outputs = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for i in range(1, len(dataset.keys())+1):\n",
    "        info = {\n",
    "            \"question\" : dataset[str(i)],\n",
    "            \"answer\" : None,\n",
    "            \"kc\": None\n",
    "        }\n",
    "    \n",
    "        prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" You are a Singaporean primary school student taking the Primary School Leaving Examination (PSLE).\n",
    "            I will give you a question which you should try to mimic a student's common mistake. The criteria for a good \"wrong\" question are as follows:\n",
    "\n",
    "            - Task-oriented dimensions (wrong solution):\n",
    "                Is wrong solution\n",
    "                Type of error is correct\n",
    "                \n",
    "            - Linguistic dimension:\n",
    "                Varied understandability for different solutions\n",
    "                Grammar not always correct\n",
    "                Fluency, clarity, conciseness not always met\n",
    "\n",
    "            Your response should follow this structure:\n",
    "\n",
    "            Solution:\n",
    "\n",
    "            (Your solution)\n",
    "\n",
    "            Error explanation:\n",
    "\n",
    "            (Your explanation of why the above solution is wrong)\n",
    "    \n",
    "            The question is: {info[\"question\"]}\n",
    "    \n",
    "    \"\"\"\n",
    "        }\n",
    "    \n",
    "        try:\n",
    "            if model is None:\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model_default,\n",
    "                        messages=[prompt],\n",
    "                        temperature=0.5\n",
    "                    )\n",
    "            elif model == 'deepseek/deepseek-r1-distill-qwen-14b':\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=[prompt],\n",
    "                        temperature=0.5,\n",
    "                        max_tokens = 30000\n",
    "                    )\n",
    "            else:\n",
    "                response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=[prompt],\n",
    "                        temperature=0.5\n",
    "                    )\n",
    "    \n",
    "            message = response.choices[0].message\n",
    "            if message is None or message.content is None:\n",
    "                print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "                continue\n",
    "    \n",
    "            response_text = message.content.strip()\n",
    "    \n",
    "            all_outputs[i] = response_text\n",
    "    \n",
    "            print(\"Question \", i, \" loaded successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call for KC: {info[\"question\"]} - {e}\")\n",
    "\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "    \n",
    "    if model is None:\n",
    "        with open(f'./solution/{model_default}_wrong_solution.json', 'w') as file:\n",
    "            json.dump(all_outputs, file, indent=4)\n",
    "    else:\n",
    "        with open(f'./solution/{model_path}_wrong_solution.json', 'w') as file:\n",
    "            json.dump(all_outputs, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769eec8-16bb-4bff-8488-ced591da590c",
   "metadata": {},
   "source": [
    "# Model evaluation for wrong questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34db177c-b1d5-4e14-9cb8-e47fdfacc0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently evaluating model:  openai/chatgpt-4o-latest\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  anthropic/claude-3.7-sonnet\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  deepseek/deepseek-r1\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'n', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  deepseek/deepseek-r1-distill-qwen-14b\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  microsoft/phi-4-reasoning-plus\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  microsoft/phi-3.5-mini-128k-instruct\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 4, 'grammar_correct': 'y', 'fluency': 4} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  qwen/qwen3-235b-a22b\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  qwen/qwen3-14b\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'n', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  meta-llama/llama-4-scout\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'n', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  23  loaded successfully\n",
      "Question  24  loaded successfully\n",
      "Currently evaluating model:  meta-llama/llama-3.3-70b-instruct\n",
      "Question  1  loaded successfully\n",
      "{'given_info_correct': 'y', 'kc_correct': 'n', 'final_ans_correct': 'n', 'error_type_correct': 'y', 'understand': 5, 'grammar_correct': 'y', 'fluency': 5} <class 'dict'>\n",
      "Question  2  loaded successfully\n",
      "Question  3  loaded successfully\n",
      "Question  4  loaded successfully\n",
      "Question  5  loaded successfully\n",
      "Question  6  loaded successfully\n",
      "Question  7  loaded successfully\n",
      "Question  8  loaded successfully\n",
      "Question  9  loaded successfully\n",
      "Question  10  loaded successfully\n",
      "Question  11  loaded successfully\n",
      "Question  12  loaded successfully\n",
      "Question  13  loaded successfully\n",
      "Question  14  loaded successfully\n",
      "Question  15  loaded successfully\n",
      "Question  16  loaded successfully\n",
      "Question  17  loaded successfully\n",
      "Question  18  loaded successfully\n",
      "Question  19  loaded successfully\n",
      "Question  20  loaded successfully\n",
      "Question  21  loaded successfully\n",
      "Question  22  loaded successfully\n",
      "Question  24  loaded successfully\n"
     ]
    }
   ],
   "source": [
    "all_evaluations_summary = {}\n",
    "\n",
    "for model in models:\n",
    "    print(\"Currently evaluating model: \", model)\n",
    "\n",
    "    model_path = model.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./solution/{model_path}_wrong_solution.json', 'r') as file:\n",
    "        all_outputs = json.load(file)\n",
    "\n",
    "    evaluation = {}\n",
    "    errors = {}\n",
    "    \n",
    "    # Assuming primary_df is defined elsewhere\n",
    "    for i in range(1, len(dataset.keys())+1):\n",
    "        info = {\n",
    "            \"question\" : dataset[str(i)],\n",
    "            \"answer\" : all_outputs.get(str(i), None),\n",
    "            \"kc\": None\n",
    "        }\n",
    "\n",
    "        if info[\"answer\"] is None:\n",
    "            continue\n",
    "    \n",
    "        prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" You are an expert educator and your role is to grade the student's answer. \n",
    "            You are given the question, the working example which consists of 2 parts: Solution, and error explanations (if it exists)\n",
    "            You need to grade them based on these rubrics:\n",
    "    \n",
    "            -Solution rubrics:\n",
    "                Given information is correct: y/n\n",
    "                Knowledge component (KC) applied is correct: y/n\n",
    "                Final answer is correct: y/n\n",
    "\n",
    "            -Error explanation rubrics:\n",
    "                Correct type of error: y/n\n",
    "    \n",
    "            -Linguistic dimension:\n",
    "                Understandable: 1 to 5\n",
    "                Grammatically correct: y/n\n",
    "                Fluency, clarity, conciseness: 1 to 5\n",
    "    \n",
    "            The question is: {info[\"question\"]}\n",
    "    \n",
    "            The working example is: {info[\"answer\"]}\n",
    "    \n",
    "            Your response should be in the json format as below, without any other comments, use strings 'y' or 'n' for yes/no:\n",
    "            {{'given_info_correct' : ,\n",
    "                'kc_correct' : ,\n",
    "                'final_ans_correct' : ,\n",
    "                'error_type_correct' : ,\n",
    "                'understand' : ,\n",
    "                'grammar_correct' : ,\n",
    "                'fluency' :\n",
    "            }}\n",
    "                \n",
    "    \n",
    "    \"\"\"\n",
    "        }\n",
    "        \n",
    "        response = evaluator.chat.completions.create(\n",
    "                model=model_evaluator,\n",
    "                messages=[prompt],\n",
    "                temperature=0.5\n",
    "            )\n",
    "    \n",
    "        message = response.choices[0].message\n",
    "        if message is None or message.content is None:\n",
    "            print(f\"No valid message returned for question: {info[\"question\"]}\")\n",
    "            continue\n",
    "    \n",
    "        response_text = message.content.strip()\n",
    "    \n",
    "        try:\n",
    "            response_dct = ast.literal_eval(response_text)\n",
    "        except Exception as e:\n",
    "            errors[i] = response_text\n",
    "            print(\"Found error at\", i, \"error: \", e)\n",
    "            print(response_text)\n",
    "    \n",
    "        evaluation[i] = response_dct\n",
    "    \n",
    "        print(\"Question \", i, \" loaded successfully\")\n",
    "    \n",
    "        if i == 1:\n",
    "            print(evaluation[i], type(evaluation[i]))\n",
    "\n",
    "    model_evaluator_path = model_evaluator.replace(\"/\",\"_\")\n",
    "\n",
    "    with open(f'./evaluation/{model_evaluator_path}_evaluating_wrong_{model_path}.json', 'w') as file:\n",
    "        json.dump(evaluation, file, indent=4)\n",
    "\n",
    "    if errors:\n",
    "        with open(f'./error/{model_evaluator_path}_evaluating_wrong_{model_path}.json', 'w') as file:\n",
    "            json.dump(errors, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1c0029-0822-45c0-929e-05b5b1effa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_chatgpt-4o-latest_evaluating_wrong_anthropic_claude-3.7-sonnet.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_deepseek_deepseek-r1-distill-qwen-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_deepseek_deepseek-r1.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_meta-llama_llama-3.3-70b-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_meta-llama_llama-4-scout.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_microsoft_phi-3.5-mini-128k-instruct.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_microsoft_phi-4-reasoning-plus.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_openai_chatgpt-4o-latest.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_qwen_qwen3-14b.json analyzed successfully\n",
      "openai_chatgpt-4o-latest_evaluating_wrong_qwen_qwen3-235b-a22b.json analyzed successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "all_evaluations_summary = {}\n",
    "\n",
    "folder_path = Path(\"./evaluation/wrong solution\")\n",
    "\n",
    "for json_file in folder_path.glob(\"*.json\"):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            evaluation = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse {json_file.name}: {e}\")\n",
    "        \n",
    "    evaluation_name = json_file.name\n",
    "\n",
    "    evaluation_summary = get_evaluation_score(evaluation)\n",
    "\n",
    "\n",
    "    given_info_score, kc_score, final_ans_score, understand_score, fluency_score, grammar_score, error_flag = evaluation_summary\n",
    "\n",
    "    if error_flag:\n",
    "        print(f\"WARNING: {evaluation_name} contains erronous evaluation\")\n",
    "    else:\n",
    "        print(f\"{evaluation_name} analyzed successfully\")\n",
    "    \n",
    "    all_evaluations_summary[evaluation_name] = {\"given_info_score\" : given_info_score, \"kc_score\":kc_score, \"final_ans_score\":final_ans_score,\n",
    "                                                \"understand_score\" : understand_score, \"fluency_score\":fluency_score, \"grammar_score\":grammar_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d450a7-f965-42e8-bc19-69b2ddedb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./final_wrong_summary.json\", \"w\") as file:\n",
    "    json.dump(all_evaluations_summary, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13905c-7bc6-41b0-9508-2ce83d0bac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A*Star",
   "language": "python",
   "name": "astar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
