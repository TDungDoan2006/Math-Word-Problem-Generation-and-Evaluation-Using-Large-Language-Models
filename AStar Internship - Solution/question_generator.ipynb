{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e5d533-521b-47f8-9980-f87a9ca006c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important libraries\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"AStarPrivate\")\n",
    "if TOKEN is None:\n",
    "    raise RuntimeError(\"Token parsing failed\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=TOKEN,\n",
    ")\n",
    "\n",
    "models = [\"openai/gpt-4.1\", \"meta-llama/llama-4-scout\", \"deepseek/deepseek-r1\", \n",
    "          \"microsoft/phi-4\", \"qwen/qwen3-14b\", \"anthropic/claude-3.7-sonnet\"]\n",
    "\n",
    "dataset = pd.read_excel(\"./dataset/all-l1-kcs.xlsx\", sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74d9a56-75fb-4a99-8d41-53c5995cc0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade</th>\n",
       "      <th>Grade_ID</th>\n",
       "      <th>Sub_strand</th>\n",
       "      <th>Order</th>\n",
       "      <th>Level_1</th>\n",
       "      <th>Level_1_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Primary 1</td>\n",
       "      <td>P1</td>\n",
       "      <td>WHOLE NUMBERS</td>\n",
       "      <td>1</td>\n",
       "      <td>Numbers up to 100</td>\n",
       "      <td>N100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primary 1</td>\n",
       "      <td>P1</td>\n",
       "      <td>WHOLE NUMBERS</td>\n",
       "      <td>8</td>\n",
       "      <td>Addition and Subtraction</td>\n",
       "      <td>AddSub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Primary 1</td>\n",
       "      <td>P1</td>\n",
       "      <td>WHOLE NUMBERS</td>\n",
       "      <td>15</td>\n",
       "      <td>Multiplication and Division</td>\n",
       "      <td>MulDiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Primary 1</td>\n",
       "      <td>P1</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>19</td>\n",
       "      <td>Money</td>\n",
       "      <td>Money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Primary 2</td>\n",
       "      <td>P2</td>\n",
       "      <td>WHOLE NUMBERS</td>\n",
       "      <td>1</td>\n",
       "      <td>Numbers up to 1000</td>\n",
       "      <td>N1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Grade Grade_ID     Sub_strand  Order                      Level_1  \\\n",
       "0  Primary 1       P1  WHOLE NUMBERS      1            Numbers up to 100   \n",
       "1  Primary 1       P1  WHOLE NUMBERS      8     Addition and Subtraction   \n",
       "2  Primary 1       P1  WHOLE NUMBERS     15  Multiplication and Division   \n",
       "3  Primary 1       P1          MONEY     19                        Money   \n",
       "4  Primary 2       P2  WHOLE NUMBERS      1           Numbers up to 1000   \n",
       "\n",
       "  Level_1_short  \n",
       "0          N100  \n",
       "1        AddSub  \n",
       "2        MulDiv  \n",
       "3         Money  \n",
       "4         N1000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"All-Level-1-KCs\"]\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c41dc67-3f56-455b-95b5-e2f4d02137c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kc_ID(row):\n",
    "    return (row['Grade_ID'] + row['Level_1_short'])\n",
    "\n",
    "dataset['kcID'] = dataset.apply(get_kc_ID, axis = 1)\n",
    "dataset_dct = dataset.set_index('kcID').to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9ed4a9-b013-4374-8580-ae5441842b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Grade': 'Primary 1', 'Grade_ID': 'P1', 'Sub_strand': 'WHOLE NUMBERS', 'Order': 1, 'Level_1': 'Numbers up to 100', 'Level_1_short': 'N100'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dct['P1N100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2db1b8-805b-4555-9325-ef78f724124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working with model:  openai/gpt-4.1\n",
      "Knowledge component P1N100 (1/32) loaded successfully\n",
      "Knowledge component P1AddSub (2/32) loaded successfully\n",
      "Knowledge component P1MulDiv (3/32) loaded successfully\n",
      "Knowledge component P1Money (4/32) loaded successfully\n",
      "Knowledge component P2N1000 (5/32) loaded successfully\n",
      "Knowledge component P2FracW (6/32) loaded successfully\n",
      "Knowledge component P2FracAS (7/32) loaded successfully\n",
      "Knowledge component P3N10k (8/32) loaded successfully\n",
      "Knowledge component P3FracEq (9/32) loaded successfully\n",
      "Knowledge component P4N100k (10/32) loaded successfully\n",
      "Knowledge component P4FctorM (11/32) loaded successfully\n",
      "Knowledge component P44Op (12/32) loaded successfully\n",
      "Knowledge component P4FracMI (13/32) loaded successfully\n",
      "Knowledge component P4FracSet (14/32) loaded successfully\n",
      "Knowledge component P4Deci3d (15/32) loaded successfully\n",
      "Knowledge component P4DeciAS (16/32) loaded successfully\n",
      "Knowledge component P4DeciMD (17/32) loaded successfully\n",
      "Knowledge component P5N1m (18/32) loaded successfully\n",
      "Knowledge component P5FracDv (19/32) loaded successfully\n",
      "Knowledge component P5Frac4Op (20/32) loaded successfully\n",
      "Knowledge component P5Deci4Op (21/32) loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\$'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge component P5perctg (22/32) loaded successfully\n",
      "Knowledge component P5rate (23/32) loaded successfully\n",
      "Knowledge component P6ratio (24/32) loaded successfully\n",
      "Knowledge component P6algebr (25/32) loaded successfully\n",
      "Knowledge component O1NumOps (26/32) loaded successfully\n",
      "Knowledge component O1RioPro (27/32) loaded successfully\n",
      "Knowledge component O1RatSpd (28/32) loaded successfully\n",
      "Knowledge component O1AgbrEF (29/32) loaded successfully\n",
      "Knowledge component O1EqIneq (30/32) loaded successfully\n",
      "Knowledge component O2Prob (31/32) loaded successfully\n",
      "Knowledge component O3NumOps (32/32) loaded successfully\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './dataset/2025-06-09 14:12:05.032117_generated_questions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m         json.dump(all_outputs_dct, file, indent=\u001b[32m4\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./dataset/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_generated_questions.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     79\u001b[39m         json.dump(all_outputs_dct, file, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AStar\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: './dataset/2025-06-09 14:12:05.032117_generated_questions.json'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "model = models[0]\n",
    "print(\"Currently working with model: \", model)\n",
    "kc_index = 0\n",
    "\n",
    "#Replace / as _ so it wouldn't interfere with path directory\n",
    "model_path = model.replace(\"/\",\"_\")\n",
    "\n",
    "all_outputs_dct = {}\n",
    "all_outputs_text = {}\n",
    "errors = {}\n",
    "\n",
    "# Assuming primary_df is defined elsewhere\n",
    "#for i in range(1, len(dataset.keys())+1):\n",
    "for kcID in dataset_dct.keys():\n",
    "    kc_index += 1\n",
    "\n",
    "    try:\n",
    "        with open(f\"./prompt/question/gpt_system_prompt.txt\",\"r\") as file:\n",
    "            system_prompt = file.read()\n",
    "    except Exception as e:\n",
    "        system_prompt = \"\"\n",
    "    try:\n",
    "        with open(f\"./prompt/question/gpt_user_prompt.txt\",\"r\") as file:\n",
    "            user_prompt = file.read()\n",
    "    except Exception as e:\n",
    "        user_prompt = \"\"\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : user_prompt.format(substrand = dataset_dct[kcID]['Sub_strand'], kc = dataset_dct[kcID]['Level_1'], \n",
    "                                           grade = dataset_dct[kcID]['Grade_ID'], ID = kcID)\n",
    "        }\n",
    "    ]\n",
    "              \n",
    "    response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            temperature=0.9\n",
    "        )\n",
    "\n",
    "    message = response.choices[0].message\n",
    "    if message is None or message.content is None:\n",
    "        print(f\"No valid message returned for kc: {dataset_dct[kcID]}\")\n",
    "        continue\n",
    "\n",
    "    response_text = message.content.strip()\n",
    "\n",
    "    response_text_extracted = None\n",
    "    match = re.findall(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if match:\n",
    "        response_text_extracted = match[0]\n",
    "    else:\n",
    "        print(\"NO JSON FORMAT OUTPUT FOUND! ERROR!\")\n",
    "        print(response_text)\n",
    "\n",
    "    try:\n",
    "        response_dct = ast.literal_eval(response_text_extracted)\n",
    "        all_outputs_dct[kcID] = response_dct\n",
    "    except Exception as e:\n",
    "        errors[i] = response_text\n",
    "        print(\"Found error at\", i, \"error: \", e)\n",
    "        print(response_text)\n",
    "\n",
    "    all_outputs_text[kcID] = response_text\n",
    "\n",
    "    print(f\"Knowledge component {kcID} ({kc_index}/{len(dataset_dct.keys())}) loaded successfully\")\n",
    "\n",
    "if model is None:\n",
    "    with open(f'./dataset/default_model_generated_questions.json', 'w') as file:\n",
    "        json.dump(all_outputs_dct, file, indent=4)\n",
    "else:\n",
    "    with open(f'./dataset/{str(datetime.datetime.now())[:-7].replace(\":\",\"_\").replace(\" \",\"_\")}_generated_questions.json', 'w') as file:\n",
    "        json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9588804-36d4-45ed-aa00-f976ffbbab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./dataset/{str(datetime.datetime.now())[:-7].replace(\":\",\"_\").replace(\" \",\"_\")}_generated_questions.json', 'w') as file:\n",
    "        json.dump(all_outputs_dct, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786149f7-c5b3-459a-9e1c-44da5f81324b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-06-09_14_16_03'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.datetime.now())[:-7].replace(\":\",\"_\").replace(\" \",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64864d1-3075-4321-b586-b92455e91085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A*Star",
   "language": "python",
   "name": "astar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
