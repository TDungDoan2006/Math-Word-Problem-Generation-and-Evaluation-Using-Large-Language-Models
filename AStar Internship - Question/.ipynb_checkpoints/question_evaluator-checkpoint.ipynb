{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3b08e6-61d8-4a27-bcb7-3b9f784c8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from api_utils import *\n",
    "import jiter\n",
    "from extract import get_next_question_id\n",
    "import re\n",
    "import json\n",
    "\n",
    "claude4_model_id = \"apac.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "client = create_client()\n",
    "openai_client = create_openai_client()\n",
    "aws_client = get_aws_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146c490",
   "metadata": {},
   "source": [
    "# Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492f7674-fb8a-4af7-802a-1307e7a192dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MWP(BaseModel):\n",
    "    answerability: int\n",
    "    primary_kc_alignment: int\n",
    "    secondary_kc_alignment: int\n",
    "    topic_alignment: int\n",
    "    grade_alignment: int\n",
    "    real_world_feasibility: int\n",
    "    synergy: int\n",
    "    clarity: int\n",
    "    conciseness_and_relevance: int\n",
    "    language_quality: int\n",
    "    content_appropriateness: int\n",
    "    explanation: str\n",
    "\n",
    "    '''  \"answerability\": 0/1,\n",
    "  \"primary_kc_alignment\": 0/1,\n",
    "  \"secondary_kc_alignment\": 0/1,\n",
    "  \"synergy\": 0/1,\n",
    "  \"topic_alignment\": 0/1,\n",
    "  \"grade_alignment\": 0/1,\n",
    "  \"real_world_feasibility\": 0/1,\n",
    "  \"clarity\": 0/1,\n",
    "  \"conciseness_and_relevance\": 0/1,\n",
    "  \"language_quality: 0/1,\n",
    "  \"content_appropriateness\": 0/1,\n",
    "  \"solution_correctness\": 0/1,'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7076b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role and Objective:\n",
      "You are a strict educational content evaluator specialized in **EVALUATE THE QUALITY OF MATH WORD PROBBLEMS ** following Singapore math syllabus. \n",
      "Your goal is to evaluate a **GIVEN MATH WORD PROBLEM** as strictly as possible based on the given **EVALUATION CRITERIA**. The math word problem is created using an intended Primary Knowledge Component, an intended Secondary Knowledge Component, an intended Topic and an intended Grade. \n",
      "\n",
      "Evaluation Criteria: \n",
      "1. Answerability: The word problem must be answerable with NO missing information, NO conflicting information and NO illogical relationship. \n",
      "2. Primary Knowledge Component Alignment: The word problem MUST be on the intended Primary Knowledge Component, that is, the Primary Knowledge Component is needed for solving the word problem. The Primary Knowledge Component decides the main idea and difficulty level of the word problem and there should be no other knowledge component in the word problem that is harder than it. \n",
      "3. Secondary Knowledge Component Alignment: The word problem MUST also target the intended Secondary Knowledge Component, that is, the Secondary Knowledge Component is also needed for solving the word problem. The Secondary Knowledge Component adds complexity to the word problem.\n",
      "4. Synergy: Primary Knowledge Component and Secondary Knowledge Component need to be used in tandem to solve the word problem. If there are multiple questions in the word problem, the questions need to be connected with each other such that the result of an early question should be used as known information to a later question in the word problem. \n",
      "5. Topic Alignment: The context of the word problem MUST belong to the given Topic. \n",
      "6. Grade Alignment: The word problem MUST be suitable for the given Student Grade. It should not require any Knowledge Components above the given Grade. \n",
      "7. Real-World Plausibility: The word problem MUST describe a realistic scenario mirroring how people typically act and operate in reality. \n",
      "8. Clarity: The language used in the word problem must be easy to understand and free from ambiguity. \n",
      "9. Conciseness and Relevance: There is no irrelevant information in the word problem that is not useful for solving the problem. All given numbers must be necessary for solving the word problem. \n",
      "10. Language Quality: The word problem uses correct grammar, spelling, and vocabulary. \n",
      "11. Content Appropriateness: The word problem is respectful, age-appropriate, and free from offensive content.\n",
      "\n",
      "Instructions: \n",
      "1. Evaluate the given math word problem as strictly as possible based on the given evaluation criteria. \n",
      "2. On each evaluation dimension, if you think the **GIVEN MATH WORD PROBLEM** fully satisfies the criteria on the dimension, rate 1, otherwise rate 0. \n",
      "3. Noted that the **MATH WORD PROBLEM** is written **IN LATEX**, ignore all the LaTeX error.\n",
      "4. Provide explanations for **ALL** criteria you gave 0, using alpha-numerical characters and +,-,*,/ only. \n",
      "\n",
      "Use the inputs below and output your evaluation (1 for fully satisfying the given criteria, otherwise 0) in a valid JSON object with keys: \"answerability\" (binary, 0/1), \"primary_kc_alignment\" (binary, 0/1),  \"secondary_kc_alignment\" (binary, 0/1), \"synergy\" (binary, 0/1), \"topic_alignment\" (binary, 0/1), \"grade_alignment\" (binary, 0/1), \"real_world_feasibility\" (binary, 0/1), \"clarity\" (binary, 0/1),  \"conciseness_and_relevance\" (binary, 0/1), \"clarity\" (binary, 0/1), \"language_quality\" (binary, 0/1), \"content_appropriateness\" (binary, 0/1), and \"explanation\" (str, explanations for any criteria you gave a negative rating of 0, leave blank otherwise). \n",
      "\n",
      "Evaluate the given Math Word Problem below which is created using the intended knowledge components, topic and grade below:\n",
      "1. The Given Math Word Problem is {word_problem}\n",
      "2. The Intended Primary Knowledge Component is {kc1}\n",
      "3. The Intended Secondary Knowledge Component is {kc2}\n",
      "4. The Intended Topic is {topic}\n",
      "5. The Intended Student Grade is {grade}\n"
     ]
    }
   ],
   "source": [
    "sys_prompt_path = r\"Prompts\\GPT\\evaluation_system_prompt_v4.1.txt\"\n",
    "with open(sys_prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    sys_prompt = file.read()\n",
    "    \n",
    "user_prompt_path = r\"Prompts\\GPT\\evaluation_user_prompt_v4.txt\"\n",
    "with open(user_prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    user_prompt_template = file.read()\n",
    "\n",
    "claude_prompt_path = r\"Prompts\\Claude\\evaluation_prompt_v2.txt\"\n",
    "with open(claude_prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    claude_prompt_template = file.read()\n",
    "    print(claude_prompt_template)\n",
    "\n",
    "def metadata(response_dict=None, **kwargs):\n",
    "    kwargs['response_dict'] = response_dict\n",
    "    return kwargs\n",
    "\n",
    "def question_evaluator(word_problem, solution, kc1, kc2, topic, grade, qid, model = \"GPT\"):\n",
    "    if (word_problem is None) or (word_problem == \"\"):\n",
    "        response_dict = {\n",
    "            \"answerability\": 0,\n",
    "            \"primary_kc_alignment\": 0,\n",
    "            \"secondary_kc_alignment\": 0,\n",
    "            \"topic_alignment\": 0,\n",
    "            \"grade_alignment\": 0,\n",
    "            \"real_world_feasibility\": 0,\n",
    "            \"synergy\": 0,\n",
    "            \"clarity_score\": 0,\n",
    "            \"conciseness_and_relevance\": 0,\n",
    "            \"language_quality_score\": 0,\n",
    "            \"content_appropriateness\": 0,\n",
    "            \"solution_correctness\": 0,\n",
    "            \"explanation\": \"No word problem is found\"\n",
    "        }\n",
    "    else:\n",
    "        if model == \"GPT4.1\":\n",
    "            user_prompt = user_prompt_template.format(word_problem = word_problem, kc1=kc1, kc2=kc2, topic=topic, grade=grade)\n",
    "            response_dict = get_GPT_struct_response(openai_client, sys_prompt, user_prompt, MWP)\n",
    "        elif model == \"Claude4\":\n",
    "            claude_prompt = claude_prompt_template.format(word_problem = word_problem, kc1 = kc1, kc2 = kc2, topic = topic, grade=grade)\n",
    "            response_dict = get_claude_response(client = aws_client, model_id = claude4_model_id, sys_prompt = None, user_prompt = claude_prompt, config={})\n",
    "            \n",
    "            # Extract the JSON block from the triple backticks\n",
    "            match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", response_dict[\"response\"], re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                extracted_dict = json.loads(json_str)\n",
    "            else:\n",
    "                print(\"No JSON found\")\n",
    "\n",
    "            # Save back into response dict\n",
    "            for key, value in extracted_dict.items():\n",
    "                response_dict[key] = value\n",
    "            response_dict.pop(\"response\",\"Text response already popped\")\n",
    "        else:\n",
    "            print(\"This model is not supported yet. Choose GPT/Claude to continue\")\n",
    "            return\n",
    "    return metadata(response_dict=response_dict, Word_problem = word_problem, Solution = solution, Primary_kc = kc1, Secondary_kc = kc2, Topic=topic, Grade=grade, QID=qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a84f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 0, 'response_time': 13.63425350189209, 'answerability': 0, 'primary_kc_alignment': 0, 'secondary_kc_alignment': 0, 'synergy': 0, 'topic_alignment': 1, 'grade_alignment': 1, 'real_world_feasibility': 1, 'clarity': 1, 'conciseness_and_relevance': 0, 'language_quality': 1, 'content_appropriateness': 1, 'explanation': 'Answerability: Missing information about factory operating hours per day makes the problem unsolvable. Primary KC Alignment: Problem requires dividing mixed number 3 1/2 by whole number 5, not dividing proper fraction by whole number as intended. Secondary KC Alignment: While fraction subtraction occurs, the synergy is missing. Synergy: Parts (a) and (b) are not connected - part (b) uses fixed subtraction 1 1/4 instead of building on part (a) result. Conciseness: The 3/4 metre per bag information is irrelevant to solving either question.'}\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "response= question_evaluator( \"\\nA factory produces \\\\( 3\\\\frac{1}{2} \\\\) metres of fabric each hour. In one day, the factory needs to use \\\\( \\\\frac{3}{4} \\\\) of a metre of fabric to make each bag.\\n\\n(a) If the factory wants to pack all of today's fabric equally into 5 boxes, how many metres of fabric will each box have?\\n\\n(b) After packing, the manager realises that \\\\(1\\\\frac{1}{4}\\\\) metres has already been used for samples. How much fabric is left in each box after the samples have been subtracted?\\n\", \n",
    "                              \"\\n(a) Total fabric produced in one hour is \\\\( 3\\\\frac{1}{2} = \\\\frac{7}{2} \\\\) metres.\\n\\nIf this fabric is packed equally into 5 boxes, the amount in each box is:\\n\\n\\\\[\\n\\\\frac{7}{2} \\\\div 5 = \\\\frac{7}{2} \\\\times \\\\frac{1}{5} = \\\\frac{7}{10}\\n\\\\]\\n\\nSo, each box has \\\\(\\\\frac{7}{10}\\\\) metres of fabric.\\n\\n(b) If \\\\(1\\\\frac{1}{4}\\\\) metres is used for samples, subtract this from the amount in each box:\\n\\nFirst, convert \\\\(1\\\\frac{1}{4}\\\\) to an improper fraction:\\n\\\\[\\n1\\\\frac{1}{4} = \\\\frac{5}{4}\\n\\\\]\\n\\nSubtract from \\\\(\\\\frac{7}{10}\\\\) (find a common denominator, which is 20):\\n\\n\\\\[\\n\\\\frac{7}{10} = \\\\frac{14}{20}, \\\\quad \\\\frac{5}{4} = \\\\frac{25}{20}\\n\\\\]\\n\\\\[\\n\\\\frac{14}{20} - \\\\frac{25}{20} = -\\\\frac{11}{20}\\n\\\\]\\n\\nSince \\\\(-\\\\frac{11}{20}\\\\) is negative, it means there is not enough fabric in each box after the samples are taken; each box is short of \\\\(\\\\frac{11}{20}\\\\) metre.\\n\", \n",
    "                              \"FRACTIONS | Division | dividing a proper fraction by a whole number\", \n",
    "                              \"FRACTIONS | Subtraction | subtracting fractions\", \n",
    "                              \"Manufacturing\",\n",
    "                              \"Primary 6\",\n",
    "                              \"P6-FrDivPN_P5-FrSubMix_GPT4.1_Manufacturing_05\",\n",
    "                              \"Claude\")\n",
    "print(response.get(\"response_dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661bc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_question_evaluator(all_responses, input_path, output_path, model = \"GPT\"):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "    question_tracker = 0\n",
    "    for qid, data_dict in input_data.items():\n",
    "        question_tracker += 1\n",
    "        print(f\"\\rCurrent question: {qid} ({question_tracker}/{len(input_data)})\", \" \"*50, end = \"\")\n",
    "        word_problem = data_dict.get(\"response_dict\", {}).get(\"word_problem\", \"\")\n",
    "        solution = data_dict.get(\"response_dict\", {}).get(\"solution\", \"\")\n",
    "        kc1 = data_dict.get(\"Primary_kc\", \"\")\n",
    "        kc2 = data_dict.get(\"Secondary_kc\", \"\")\n",
    "        topic = data_dict.get(\"Topic\", \"\")\n",
    "        grade = data_dict.get(\"Grade\", \"\")\n",
    "        response = question_evaluator(word_problem = word_problem, solution = solution, kc1 = kc1, kc2 = kc2, topic = topic, grade = grade, qid = qid, model = model)\n",
    "        if response:  \n",
    "            all_responses[\"Question \" + str(question_tracker)] = response\n",
    "            all_responses = exercise_NA_evaluation(all_responses)\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_responses, f, indent=2)\n",
    "\n",
    "def batch_split_question_evaluator(all_responses, input_path, output_path, model = \"GPT\"):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "\n",
    "    evaluator_tracker = 0\n",
    "    for evaluator, question_set in input_data.items():\n",
    "        evaluator_tracker += 1\n",
    "        question_tracker = 0\n",
    "        all_responses[evaluator] = {}\n",
    "        for qid, data_dict in question_set.items():\n",
    "            question_tracker += 1\n",
    "            print(f\"\\rCurrent model: {model}, Current corresponding human evaluator: {evaluator} ({evaluator_tracker}/{len(input_data)}) Current question: {qid} ({question_tracker}/{len(question_set)})\" + \" \"*50, end = \" \")\n",
    "            word_problem = data_dict.get(\"response_dict\", {}).get(\"word_problem\", \"\")\n",
    "            solution = data_dict.get(\"response_dict\", {}).get(\"solution\", \"\")\n",
    "            kc1 = data_dict.get(\"Primary_kc\", \"\")\n",
    "            kc2 = data_dict.get(\"Secondary_kc\", \"\")\n",
    "            topic = data_dict.get(\"Topic\", \"\")\n",
    "            grade = data_dict.get(\"Grade\", \"\")\n",
    "            response = question_evaluator(word_problem = word_problem, solution = solution, kc1 = kc1, kc2 = kc2, topic = topic, grade = grade, qid = qid, model = model)\n",
    "            if response:  \n",
    "                all_responses[evaluator][\"Question \" + str(question_tracker)] = response\n",
    "                all_responses = exercise_NA_evaluation(all_responses)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(all_responses, f, indent=2)\n",
    "    return all_responses\n",
    "\n",
    "def exercise_NA_evaluation(all_responses):\n",
    "    depth = 1\n",
    "    next_layer = all_responses.get(list(all_responses.keys())[0])\n",
    "    while type(next_layer) == dict:\n",
    "        depth += 1\n",
    "        next_layer = next_layer.get(list(next_layer.keys())[0])\n",
    "\n",
    "    if depth == 3:\n",
    "        for evaluator, question_list in all_responses.items():\n",
    "            for qid, evaluation in question_list.items():\n",
    "                response = evaluation[\"response_dict\"]\n",
    "                if response[\"answerability\"] == 0:\n",
    "                    response[\"primary_kc_alignment\"] = 'NA'\n",
    "                    response[\"secondary_kc_alignment\"] = 'NA'\n",
    "                    response[\"topic_alignment\"] = 'NA'\n",
    "                    response['grade_alignment'] = 'NA'\n",
    "                    response[\"real_world_feasibility\"] = 'NA'\n",
    "                    response[\"synergy\"] = 'NA'\n",
    "                    response[\"clarity\"] = 'NA'\n",
    "                    response['conciseness_and_relevance'] = 'NA'\n",
    "                    response[\"language_quality\"] = 'NA'\n",
    "                    response[\"content_appropriateness\"] = 'NA'\n",
    "                if response[\"primary_kc_alignment\"] == 0:\n",
    "                    response['grade_alignment'] = 'NA'\n",
    "                    response[\"synergy\"] = 'NA'\n",
    "                if response[\"secondary_kc_alignment\"] == 0:\n",
    "                    response[\"synergy\"] = 'NA'\n",
    "    elif depth == 2:\n",
    "         for qid, evaluation in all_responses.items():\n",
    "            response = evaluation[\"response_dict\"]\n",
    "            if response[\"answerability\"] == 0:\n",
    "                response[\"primary_kc_alignment\"] = 'NA'\n",
    "                response[\"secondary_kc_alignment\"] = 'NA'\n",
    "                response[\"topic_alignment\"] = 'NA'\n",
    "                response['grade_alignment'] = 'NA'\n",
    "                response[\"real_world_feasibility\"] = 'NA'\n",
    "                response[\"synergy\"] = 'NA'\n",
    "                response[\"clarity\"] = 'NA'\n",
    "                response['conciseness_and_relevance'] = 'NA'\n",
    "                response[\"language_quality\"] = 'NA'\n",
    "                response[\"content_appropriateness\"] = 'NA'\n",
    "            if response[\"primary_kc_alignment\"] == 0:\n",
    "                response['grade_alignment'] = 'NA'\n",
    "                response[\"synergy\"] = 'NA'\n",
    "            if response[\"secondary_kc_alignment\"] == 0:\n",
    "                response[\"synergy\"] = 'NA'\n",
    "    else:\n",
    "        print(\"Wrong argument format! Please check again if it is all_responses dictionary generated by the LLM\")\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864d104a-85e6-48d1-8bf1-2e9c32c75f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation round: 1 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 2 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 3 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 4 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 5 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   "
     ]
    }
   ],
   "source": [
    "# GPT evaluating common questions in recreation\n",
    "version = 0\n",
    "for i in range(5):\n",
    "    version += 1\n",
    "    print(\"Evaluation round:\", i+1, \"/ 5\")\n",
    "    all_responses_r_common = {}\n",
    "    all_responses_r_common = batch_question_evaluator(all_responses = all_responses_r_common,\n",
    "                                                      input_path = r\"C:\\Users\\Amin\\A-Star-AI-for-education\\AStar Internship - Question\\Data\\Generated_questions\\GPT4.1\\All\\recreation\\100_recreation_v8_v1.json\",\n",
    "                                                      output_path = rf\".\\Data\\Evaluated_questions\\GPT4.1\\All\\recreation\\recreation_gpt_v2_common_{version}.json\",\n",
    "                                                      model = \"GPT\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b329db-e40b-4d70-b235-d516a3dcc811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation round: 1 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 2 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 3 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 4 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   Evaluation round: 5 / 5\n",
      "Current question: P6-RoFndRoWN_P1-WNSub2nd_GPT4.1_Recreation_01 (100/100)                                                   "
     ]
    }
   ],
   "source": [
    "# Claude evaluating common questions in recreation\n",
    "version = 0\n",
    "for i in range(5):\n",
    "    version += 1\n",
    "    all_responses_r_common = {}\n",
    "    print(\"Evaluation round:\", i+1, \"/ 5\")\n",
    "    all_responses_r_common = batch_question_evaluator(all_responses = all_responses_r_common,\n",
    "                                                      input_path = r\"C:\\Users\\Amin\\A-Star-AI-for-education\\AStar Internship - Question\\Data\\Generated_questions\\GPT4.1\\All\\recreation\\100_recreation_v8_v1.json\",\n",
    "                                                      output_path = rf\".\\Data\\Evaluated_questions\\Claude4\\All\\recreation\\recreation_gpt_v2_common_{version}.json\",\n",
    "                                                      model = \"Claude\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating all topics generated by GPT\n",
    "models = [\"GPT4.1\", \"Claude4\"]\n",
    "topics = [\"recreation\", \"services\", \"householdfinance\"]\n",
    "for evaluator_model in models:\n",
    "    for version in range(1,6):\n",
    "        print(f\"Current round: {version}/5\")\n",
    "        all_responses = {}\n",
    "        for topic in topics:\n",
    "            all_responses[topic] = {}\n",
    "            all_resposnes[topic] = batch_split_question_evaluator(all_responses = all_responses[topic], \n",
    "                                                          input_path = rf\".\\Data\\Generated_questions\\GPT4.1\\All\\{topic}\\{topic}_v8_v1_split.json\", \n",
    "                                                          output_path = rf\".\\Data\\Evaluated_questions\\{evaluator_model}\\All\\{topic}\\{topic}_gpt_v2_{version}.json\",\n",
    "                                                          model = evaluator_model\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6f6fab-660b-41e3-90f7-1ad1299caf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round: 2/5\n",
      "Current model: GPT4.1, Current corresponding human evaluator: Sarah (3/3) Current question: O3-SPMulProb_O3-SPFndPrCE_sonnet4_Recreation_04 (64/64)                                                   Current round: 3/5\n",
      "Current model: GPT4.1, Current corresponding human evaluator: Sarah (3/3) Current question: O3-SPMulProb_O3-SPFndPrCE_sonnet4_Recreation_04 (64/64)                                                   Current round: 4/5\n",
      "Current model: GPT4.1, Current corresponding human evaluator: Sarah (3/3) Current question: O3-SPMulProb_O3-SPFndPrCE_sonnet4_Recreation_04 (64/64)                                                   Current round: 5/5\n",
      "Current model: GPT4.1, Current corresponding human evaluator: Sarah (3/3) Current question: O3-SPMulProb_O3-SPFndPrCE_sonnet4_Recreation_04 (64/64)                                                   "
     ]
    }
   ],
   "source": [
    "# Evaluating all topics generated by Claude\n",
    "models = [\"GPT4.1\", \"Claude4\"]\n",
    "topics = [\"recreation\",\"services\",\"householdfinance\"] # 1 topic generated by Claude only\n",
    "for evaluator_model in models:\n",
    "    for version in range(1,6):\n",
    "        print(f\"Current round: {version}/5\")\n",
    "        all_responses = {}\n",
    "        for topic in topics:\n",
    "            all_responses[topic] = {}\n",
    "            all_responses[topic] = batch_split_question_evaluator(all_responses = all_responses[topic], \n",
    "                                                          input_path = rf\".\\Data\\Generated_questions\\Claude4\\All\\{topic}\\{topic}_split_v3.json\", \n",
    "                                                          output_path = rf\".\\Data\\Evaluated_questions\\{evaluator_model}\\All\\{topic}\\{topic}_claude_v3_{version}.json\",\n",
    "                                                          model = evaluator_model\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b41122f-afbd-4da8-95b8-0ee7d7fc02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is to turn the evaluations to NA where applicable (e.g. turn all to NA when \"answerable\" is 0)\n",
    "# Set your directory path\n",
    "directory = r\".\\Data\\Evaluated_questions\\Claude4\\All\\recreation\"\n",
    "\n",
    "# Loop through all JSON files\n",
    "file_id = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_id += 1\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Load JSON\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "    # Use the helper function defined above (same block as batch_split_question_evaluator)\n",
    "    exercise_NA_evaluation(data)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74706a71-6a20-4d16-bb40-8f788a9cef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amin\\AppData\\Local\\Temp\\ipykernel_17252\\3916404648.py:62: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(sanitize_excel_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sanitized file: recreation_claude_v2_1.xlsx\n",
      "Saved sanitized file: recreation_claude_v2_2.xlsx\n",
      "Saved sanitized file: recreation_claude_v2_3.xlsx\n",
      "Saved sanitized file: recreation_claude_v2_4.xlsx\n",
      "Saved sanitized file: recreation_claude_v2_5.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def sanitize_excel_cell(val):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    try:\n",
    "        if isinstance(val, (int, float)):\n",
    "            return val\n",
    "        val_str = str(val)\n",
    "        val_str = re.sub(r'[\\x00-\\x1F\\x7F]', '', val_str)\n",
    "        val_str = re.sub(r'\\\\[a-zA-Z]+(?:\\{.*?\\})*', '[FORMULA]', val_str)\n",
    "        if val_str == \"NA\":\n",
    "            return \"'NA\"  # Force Excel to show it as a string\n",
    "        return val_str\n",
    "    except Exception:\n",
    "        return '[INVALID]'\n",
    "\n",
    "# Set your directory path\n",
    "directory = r\".\\Data\\Evaluated_questions\\Claude4\\All\\recreation\"\n",
    "\n",
    "# Loop through all JSON files\n",
    "file_id = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_id += 1\n",
    "    if filename.endswith(\".json\") and filename.startswith(\"recreation_claude\"):\n",
    "        json_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Load JSON\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        depth = 1\n",
    "        next_layer = data.get(list(data.keys())[0])\n",
    "        while type(next_layer) == dict:\n",
    "            depth += 1\n",
    "            next_layer = next_layer.get(list(next_layer.keys())[0])\n",
    "\n",
    "        # Build row-wise records\n",
    "        rows = []\n",
    "        # Depth = 3 means the dataset is split for 3 evaluator\n",
    "        # Depth = 2 means common dataset\n",
    "        if depth == 3:\n",
    "            for evaluator, questions in data.items():\n",
    "                for question_id, question_data in questions.items():\n",
    "                    response_dict = question_data.get(\"response_dict\", {})\n",
    "                    row = {\"Evaluator\": evaluator, \"Question\": question_id}\n",
    "                    row.update(response_dict)\n",
    "                    rows.append(row)\n",
    "        else:\n",
    "            for question_id, question_data in data.items():\n",
    "                response_dict = question_data.get(\"response_dict\", {})\n",
    "                row = {\"Evaluator\": evaluator, \"Question\": question_id}\n",
    "                row.update(response_dict)\n",
    "                rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # Sanitize each cell\n",
    "        df = df.applymap(sanitize_excel_cell)\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_filename = filename.replace(\".json\", \".xlsx\")\n",
    "        excel_path = os.path.join(directory, excel_filename)\n",
    "        df.to_excel(excel_path, index=False)\n",
    "\n",
    "        print(f\"Saved sanitized file: {excel_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b864fc4-530e-4e88-9bce-4a7e260be57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added sheet: recreation_claude_v2_1\n",
      "✅ Added sheet: recreation_claude_v2_2\n",
      "✅ Added sheet: recreation_claude_v2_3\n",
      "✅ Added sheet: recreation_claude_v2_4\n",
      "✅ Added sheet: recreation_claude_v2_5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set your directory path\n",
    "directory = r\".\\Data\\Evaluated_questions\\Claude4\\All\\recreation\"\n",
    "\n",
    "# List all Excel files in the folder\n",
    "excel_files = [f for f in os.listdir(directory) if f.endswith(\".xlsx\") and f.startswith(\"recreation_claude\")]\n",
    "\n",
    "# Create a new Excel writer for the combined output\n",
    "with pd.ExcelWriter(os.path.join(directory, \"combined_output_claude.xlsx\"), engine=\"openpyxl\") as writer:\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        # Read the first (and only) sheet\n",
    "        df = pd.read_excel(file_path)\n",
    "        # Use filename (without .xlsx) as sheet name, limit to 31 chars\n",
    "        sheet_name = os.path.splitext(file)[0][:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        print(f\"✅ Added sheet: {sheet_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e305f-4fda-447d-a00b-d2064e5e7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\Amin\\A-Star-AI-for-education\\AStar Internship - Question\\Data\\Generated_questions\\GPT4.1\\All\\householdfinance\\householdfinance_v8_v1_split.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d04114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def convert_json_to_table(json_file_path, output_file_path, output_format=\"csv\"):\n",
    "    # Step 1: Load the JSON file\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Step 2: Extract relevant fields for each question\n",
    "    rows = []\n",
    "    for idx, (qid, content) in enumerate(data.items(), start=1):\n",
    "        r = content[\"response_dict\"]\n",
    "        row = {\n",
    "            \"Question no\": idx,\n",
    "            \"Answerability\": r[\"answerablitily\"],\n",
    "            \"Primary KC Alignment\": r[\"primary_kc_alignment\"],\n",
    "            \"Secondary KC Alignment\": r[\"secondary_kc_alignment\"],\n",
    "            \"Synergy\": r[\"synergy\"],\n",
    "            \"Topic Alignment\": r[\"topic_alignment\"],\n",
    "            \"Grade Alignment\": r[\"grade_alignment\"],\n",
    "            \"Real-World Feasibility\": r[\"real_world_feasibility\"],\n",
    "            \"Clarity\": r[\"clarity\"],\n",
    "            \"Conciseness and Relevance\": r[\"conciseness_and_relevance\"],\n",
    "            \"Language Quality\": r[\"language_quality\"],\n",
    "            \"Content Appropriateness\": r[\"content_appropriateness\"],\n",
    "            \"Solution Correctness\": r[\"solution_correctness\"]\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    # Step 3: Convert to DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Step 4: Save as CSV or Excel\n",
    "    if output_format == \"csv\":\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "    elif output_format == \"excel\":\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid output format. Choose 'csv' or 'excel'.\")\n",
    "\n",
    "    print(f\"File saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8e8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to O1_GPT4.1_questions_20_sample_updated_v5_v1_GPT4.1_v2.csv\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_table(r\"Data\\Evaluated_questions\\GPT4.1\\O1\\O1_GPT4.1_questions_20_sample_updated_v5_v1.json\", r\"O1_GPT4.1_questions_20_sample_updated_v5_v1_GPT4.1_v2.csv\", \"csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A*Star",
   "language": "python",
   "name": "astar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
