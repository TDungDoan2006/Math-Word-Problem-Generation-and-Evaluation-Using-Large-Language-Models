{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3b08e6-61d8-4a27-bcb7-3b9f784c8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from api_utils import *\n",
    "import jiter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146c490",
   "metadata": {},
   "source": [
    "# Generate MWPs and store them as JSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5635ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_descriptions = {\n",
    "    \"Food\": \"Food such as cooking, baking, food consumption, food sharing, food allocation, buying food, selling food etc.\",\n",
    "    \"Education\": \"Education such as stationary, library, bookstores, classrooms, events at schools, school clubs, teaching and learning activities etc.\",\n",
    "    \"Transportation\": \"Transportation such as traveling, commuting, shipping, trucking, freight, flights etc.\",\n",
    "    \"Household Finance\": \"Household finance such as income, utility bills, money, interest, savings, instalment, mortgage, financial planning etc.\",\n",
    "    \"Recreation\": \"Recreation such as sports, games, exercises, music, movie, dancing, painting, fishing and other recreation activities\",\n",
    "    \"Farming\": \"Farming such as farming of crops, vegetables, fruits, nuts, livestock, aquaculture etc.\",\n",
    "    \"Manufacturing\": \"Manufacturing such as manufacturing of textile, apparel, shoes, electronics, furniture, cars, toys, chemicals etc.\",\n",
    "    \"Services\": \"Services such as installation, maintenance, repairing, cleaning, laundry, hotel, retail, e-commerce, streaming services, digital services etc.\"}\n",
    "\n",
    "batch_path_dict = {\"P\": r\"Data\\Final_kc_pairs\\primary_kc_pairs_final.json\",\n",
    "                    \"P3\": r\"Data\\Final_kc_pairs\\P3_kc_pairs_final.json\",\n",
    "                    \"P4\": r\"Data\\Final_kc_pairs\\P4_kc_pairs_final.json\",\n",
    "                    \"P5\": r\"Data\\Final_kc_pairs\\P5_kc_pairs_final.json\",\n",
    "                    \"P6\": r\"Data\\Final_kc_pairs\\P6_kc_pairs_final.json\",\n",
    "                    \"O\": r\"Data\\Final_kc_pairs\\secondary_kc_pairs_final.json\",\n",
    "                    \"O1\": r\"Data\\Final_kc_pairs\\O1_kc_pairs_final.json\",\n",
    "                    \"O2\": r\"Data\\Final_kc_pairs\\O2_kc_pairs_final.json\",\n",
    "                    \"O3\": r\"Data\\Final_kc_pairs\\O3_kc_pairs_final.json\",\n",
    "                    \"All\": r\"Data\\Final_kc_pairs\\all_kc_pairs_final.json\",\n",
    "                    \"50\": r\"Data\\Final_kc_pairs\\50_hardest_kc_pairs_final.json\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b25bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt_path = r\"Prompts\\GPT\\generation_system_prompt_without_solution.txt\"\n",
    "with open(sys_prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    sys_prompt = file.read()\n",
    "    \n",
    "user_prompt_path = r\"Prompts\\GPT\\generation_user_prompt.txt\"\n",
    "with open(user_prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    user_prompt_template = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492f7674-fb8a-4af7-802a-1307e7a192dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = create_openai_client()\n",
    "class MWP(BaseModel):\n",
    "    word_problem: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7076b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata(response_dict=None, **kwargs):\n",
    "    kwargs['response_dict'] = response_dict\n",
    "    return kwargs\n",
    "\n",
    "def GPT_question_generator(kc1, kc2, topic, grade, qid):\n",
    "    user_prompt = user_prompt_template.format(kc1=kc1, kc2=kc2, topic=topic, grade=grade)\n",
    "    response_dict = get_GPT_struct_response(openai_client, sys_prompt, user_prompt, MWP)\n",
    "    return metadata(response_dict=response_dict, Primary_kc = kc1, Secondary_kc = kc2, Topic=topic, Grade=grade, QID=qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a84f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Primary_kc': 'WHOLE NUMBERS | Multiplication | multiplication of numbers with 1-4 digits by number with 1 digit or of number with 1-3 digits by number with 2 digits',\n",
       " 'Secondary_kc': 'WHOLE NUMBERS | Subtraction | subtraction of numbers with 1-4 digits',\n",
       " 'Topic': 'Money',\n",
       " 'Grade': 'Primary 6',\n",
       " 'QID': 123,\n",
       " 'response_dict': {'status': 0,\n",
       "  'word_problem': 'Jasmine is helping her family sell tickets for a school charity event. Each ticket costs \\\\textdollar12 and she manages to sell 27 tickets. Her school sets a target to collect \\\\textdollar350 more than what Jasmine collected from ticket sales. \\n\\n(a) How much money did Jasmine collect from selling the tickets?\\n\\n(b) How much money must her school collect in total to meet the target?\\n\\n(c) If the school has already collected \\\\textdollar500 in total, how much more do they need to reach their target amount?\\n\\nShow all your working clearly.',\n",
       "  'response_time': 4.4020836353302}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "response= GPT_question_generator( \"WHOLE NUMBERS | Multiplication | multiplication of numbers with 1-4 digits by number with 1 digit or of number with 1-3 digits by number with 2 digits\", \"WHOLE NUMBERS | Subtraction | subtraction of numbers with 1-4 digits\", \"Money\", \"Primary 6\", 123)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661bc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_GPT_question_generator(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_responses = json.load(f)\n",
    "    else:\n",
    "        all_responses = {}\n",
    "    for qid, data_dict in input_data.items():\n",
    "        kc1 = data_dict.get(\"primary_kc_name\", \"\")\n",
    "        kc2 = data_dict.get(\"secondary_kc_name\", \"\")\n",
    "        grade = data_dict.get(\"primary_kc_grade\", \"\")\n",
    "        for topic_key, topic_description in topic_descriptions.items():\n",
    "            updated_qid = qid + \"_GPT4.1_\"+ topic_key\n",
    "            updated_qid = get_next_question_id(updated_qid)\n",
    "            response = GPT_question_generator(kc1 = kc1, kc2 = kc2, topic = topic_description, grade = grade, qid = updated_qid)\n",
    "            if response:  \n",
    "                all_responses[updated_qid] = response\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315fd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_topic_GPT_question_generator(input_path, output_path, topic, with_solution=True):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_responses = json.load(f)\n",
    "    else:\n",
    "        all_responses = {}\n",
    "    for qid, data_dict in input_data.items():\n",
    "        kc1 = data_dict.get(\"primary_kc_name\", \"\")\n",
    "        kc2 = data_dict.get(\"secondary_kc_name\", \"\")\n",
    "        grade = data_dict.get(\"primary_kc_grade\", \"\")\n",
    "        updated_qid = qid + \"_GPT4.1_\"+ topic\n",
    "        if not with_solution:\n",
    "            updated_qid = updated_qid + \"_without_solution\"\n",
    "        updated_qid = get_next_question_id(updated_qid)\n",
    "        response = GPT_question_generator(kc1 = kc1, kc2 = kc2, topic = topic_descriptions[topic], grade = grade, qid = updated_qid)\n",
    "        if response:  \n",
    "            all_responses[updated_qid] = response\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7593b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_topic_GPT_question_generator(batch_path_dict[\"50\"], r\"Data\\Generated_questions\\GPT4.1\\50_hardest\\50_hardest_services_without_solution_v1.json\", \"Services\", with_solution=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc0393",
   "metadata": {},
   "source": [
    "# KC pairs slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5def27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6584dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_samples(sample_size, input_path, output_path, exclude_keys_path=None):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "    with open(exclude_keys_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        exclude_keys = json.load(f) if exclude_keys_path else None\n",
    "    input_data = {k: v for k, v in input_data.items() if exclude_keys is None or k not in exclude_keys}\n",
    "    selected_samples = random.sample(list(input_data.items()), sample_size)\n",
    "    selected_data = {qid: data for qid, data in selected_samples}\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(selected_data, f, indent=2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2d2c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_random_samples(80, r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1.json\", r\"Data\\Generated_questions\\GPT4.1\\All\\80_recreation_v8_v1.json\", exclude_keys_path=r\"Data\\Generated_questions\\GPT4.1\\All\\20_recreation_v8_v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a965fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_remaining_MWPs_into_three_parts(input_path, exclude_keys_path=None):\n",
    "    # Load input data\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "\n",
    "    # Load exclude keys if provided\n",
    "    exclude_keys = set()\n",
    "    if exclude_keys_path:\n",
    "        with open(exclude_keys_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            exclude_keys = set(json.load(f))\n",
    "\n",
    "    # Remove excluded keys\n",
    "    filtered_data = {k: v for k, v in input_data.items() if k not in exclude_keys}\n",
    "\n",
    "    # Shuffle keys for randomness\n",
    "    items = list(filtered_data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    # Split into 3 nearly equal parts\n",
    "    total = len(items)\n",
    "    split_size = total // 3\n",
    "    part1 = dict(items[:split_size])\n",
    "    part2 = dict(items[split_size:2*split_size])\n",
    "    part3 = dict(items[2*split_size:])\n",
    "\n",
    "    return part1, part2, part3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35568d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 26\n"
     ]
    }
   ],
   "source": [
    "part1, part2, part3 = split_remaining_MWPs_into_three_parts(r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1.json\", exclude_keys_path=r\"Data\\Generated_questions\\GPT4.1\\All\\100_recreation_v8_v1.json\")\n",
    "print(len(part1), len(part2), len(part3))\n",
    "dictionary = {\n",
    "    \"TD\": part1,\n",
    "    \"Minh\": part2,\n",
    "    \"Sarah\": part3\n",
    "}\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e849af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign KC pairs to each person based on the corresponding assigned MWPs in recreation_v8_v1_split.json\n",
    "input_path = r\"Data\\Generated_questions\\GPT4.1\\All\\recreation\\recreation_v8_v1_split.json\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    input_data = json.load(f)\n",
    "assigned_KCs = dict()\n",
    "for name, data in input_data.items():\n",
    "    assigned_KCs[name] = list()\n",
    "    for qid in data.keys():\n",
    "        qid = \"_\".join(qid.split(\"_\")[:2])\n",
    "        assigned_KCs[name].append(qid)\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\assigned_KCs.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "     json.dump(assigned_KCs, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are 76 KC pairs of 76 MWPs which are left after the common round of 100 MWPs evaluation\n",
    "alr_assign = [\"P3-WNSub4d_P1-WNAdd2nd\",\n",
    "    \"P5-DcDiv3dK_P4-DcRnd3d\",\n",
    "    \"P5-DcDiv3dK_P4-DcAdd2nd\",\n",
    "    \"P4-DcMul2d1d_P4-DcCnv2Fr\",\n",
    "    \"P4-DcDiv2d1d_P4-DcCmp3d\",\n",
    "    \"P3-WNMul3d1d_P1-WNCmp\",\n",
    "    \"P6-FrDivPP_P5-FrCnv2Dc\",\n",
    "    \"P4-DcSub2d_P4-DcRnd3d\",\n",
    "    \"O3-MXMul_O3-MXAdd\",\n",
    "    \"P6-PcFndWN_P1-WNAdd2nd\",\n",
    "    \"P5-FrMulImIm_P2-FrSub2nd\",\n",
    "    \"P6-AgRepLrEx_P6-AgSmpLrEx\",\n",
    "    \"P4-DcDiv2d1d_P4-DcAdd2nd\",\n",
    "    \"O1-PcFndRslt_P1-WNSub2nd\",\n",
    "    \"O3-SPFndstd_O2-SPFndmean\",\n",
    "    \"P5-DcMul3dK_P4-DcSub2nd\",\n",
    "    \"P5-FrMulPIm_P3-FrSmp\",\n",
    "    \"P5-PcRepWh_P1-WNDiv2nd\",\n",
    "    \"P6-FrDivPN_P2-FrAdd2nd\",\n",
    "    \"P5-FrMulPIm_P2-FrCmp\",\n",
    "    \"O1-PcRepRvs_O1-PcCnv2Dc\",\n",
    "    \"P4-DcAdd2d_P4-DcCmp3d\",\n",
    "    \"P5-FrMulImIm_P3-FrSmp\",\n",
    "    \"O2-RoRepDP_P1-WNMul2nd\",\n",
    "    \"P4-WNDiv4d1d_P4-WNRnd5d\",\n",
    "    \"P4-DcAdd2d_P4-DcRnd3d\",\n",
    "    \"P6-FrDivPN_P3-FrSmp\",\n",
    "    \"P5-DcDiv3dK_P4-DcCnv2Fr\",\n",
    "    \"O3-MXMulSM_O3-MXSub\",\n",
    "    \"P6-PcFndWN_P1-WNSub2nd\",\n",
    "    \"P4-FrSubU12_P2-FrCmp\",\n",
    "    \"P6-AgSlvLrN_P6-AgRepLrEx\",\n",
    "    \"P5-RtFndR_P2-DcCnvD2N\",\n",
    "    \"P5-FrMulPIm_P5-FrCnv2Dc\",\n",
    "    \"O3-SPFndQtl_O3-SPFndIQR\",\n",
    "    \"O3-SPMulProb_O2-SPRepPrSE\",\n",
    "    \"O1-RoRepFr_P6-FrDiv2nd\",\n",
    "    \"O2-RoRepIvP_P1-WNDiv2nd\",\n",
    "    \"P3-WNDiv3d1d_P1-WNAdd2nd\",\n",
    "    \"O2-AgSlvLr2v_O1-AgRepEq\",\n",
    "    \"P5-FrMulMixN_P5-FrCnv2Dc\",\n",
    "    \"P3-WNDiv3d1d_P1-WNMul2nd\",\n",
    "    \"P6-PcFndChg_P1-WNSub2nd\",\n",
    "    \"P3-FrAddRl12_P2-FrCmp\",\n",
    "    \"P4-DcDiv2d1d_P4-DcSub2nd\",\n",
    "    \"P5-RtFndR_P2-DcCnvN2D\",\n",
    "    \"P6-PcFndChg_P1-WNMul2nd\",\n",
    "    \"O3-STOprUn_O3-STOprIns\",\n",
    "    \"P3-WNDivRmd3d_P1-WNAdd2nd\",\n",
    "    \"O2-SPFndmdn_O2-SPFndmode\",\n",
    "    \"P6-PcFndWN_P1-WNMul2nd\",\n",
    "    \"O2-AgSlvIneq_O2-AgRepIneq\",\n",
    "    \"O3-SPMulProb_O3-SPFndPrCE\",\n",
    "    \"P6-FrDivPN_P2-FrSub2nd\",\n",
    "    \"O1-PcRepRvs_O1-PcCnv2Fr\",\n",
    "    \"P4-DcMul2d1d_P4-DcCmp3d\",\n",
    "    \"O1-RoRepDc_P4-DcAdd2nd\",\n",
    "    \"P5-FrMulImN_P2-FrAdd2nd\",\n",
    "    \"O1-PcRep2q_O1-PcCnv2Fr\",\n",
    "    \"P5-FrMulImIm_P5-FrCnv2Dc\",\n",
    "    \"P5-FrMulImN_P5-FrCnv2Dc\",\n",
    "    \"O1-AgRepExSq_O1-AgEvlEx\",\n",
    "    \"P5-PcRepWh_P1-WNSub2nd\",\n",
    "    \"P6-PcFndChg_P1-WNAdd2nd\",\n",
    "    \"O1-RoRepFr_P5-FrMul2nd\",\n",
    "    \"P5-FrSubMix_P5-FrCnv2Dc\",\n",
    "    \"P5-DcDiv3dK_P4-DcSub2nd\",\n",
    "    \"P6-PcFndWN_P1-WNDiv2nd\",\n",
    "    \"P4-DcMul2d1d_P4-DcSub2nd\",\n",
    "    \"O3-SPAddProb_O3-SPFndPrCE\",\n",
    "    \"O1-PcFndRslt_P1-WNAdd2nd\",\n",
    "    \"P3-WNMul3d1d_P1-WNSub2nd\",\n",
    "    \"O2-SPFndmdn_O3-SPFndrng\",\n",
    "    \"P5-DcMul3dK_P4-DcAdd2nd\",\n",
    "    \"P6-FrDivPN_P5-FrMul2nd\",\n",
    "    \"P5-DcDiv3dK_P4-DcCmp3d\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the remaining 100 KC pairs for each person\n",
    "target_sizes = [24, 38, 38]\n",
    "groups = [[] for _ in target_sizes]\n",
    "counts = [0] * 3\n",
    "\n",
    "# Index to cycle through 0 → 1 → 2 → 0 ...\n",
    "i = 0\n",
    "with open(r\"Data\\Final_kc_pairs\\all_kc_pairs_final.json\", \"r\", encoding= \"utf-8\") as f:\n",
    "    kc_pairs = json.load(f)\n",
    "remaining_kc_pairs = [k for k in kc_pairs if k not in alr_assign]\n",
    "for kc in remaining_kc_pairs:\n",
    "    while counts[i] >= target_sizes[i]:\n",
    "        i = (i + 1) % 3  # skip full group\n",
    "    groups[i].append(kc)\n",
    "    counts[i] += 1\n",
    "    i = (i + 1) % 3  # move to next group\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\assigned_KCs.json\", 'r', encoding = \"utf_8\") as f:\n",
    "    assigned_KCs = json.load(f)\n",
    "assigned_KCs[\"TD\"].extend(groups[0])\n",
    "assigned_KCs[\"Minh\"].extend(groups[1])\n",
    "assigned_KCs[\"Sarah\"].extend(groups[2])\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\assigned_KCs.json\", 'w', encoding = \"utf_8\") as f:\n",
    "    json.dump(assigned_KCs, f, indent=2, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7830ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 176 MWPs on another topics according to the assigned KC pairs.\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\services\\services_v8_v1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    MWPs = json.load(f)\n",
    "# Initialize split dictionary\n",
    "services_v8_v1_split = {\"TD\": dict(), \"Minh\": dict(), \"Sarah\": dict()}\n",
    "\n",
    "# Assign based on safe_qid matching assigned_KCs\n",
    "for qid, data in MWPs.items():\n",
    "    safe_qid = \"_\".join(qid.split(\"_\")[:2])\n",
    "    if safe_qid in assigned_KCs[\"TD\"]:\n",
    "        services_v8_v1_split[\"TD\"][qid] = data\n",
    "    elif safe_qid in assigned_KCs[\"Minh\"]:\n",
    "        services_v8_v1_split[\"Minh\"][qid] = data\n",
    "    elif safe_qid in assigned_KCs[\"Sarah\"]:\n",
    "        services_v8_v1_split[\"Sarah\"][qid] = data\n",
    "\n",
    "\n",
    "with open(r\"Data\\Generated_questions\\GPT4.1\\All\\services\\services_v8_v1_split.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(services_v8_v1_split, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac369cb",
   "metadata": {},
   "source": [
    "# Convert JSON files into TEX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d04114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "def convert_json_into_tex(input_path, output_file):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        input_data = json.load(f)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\\\documentclass{article}\\n\")\n",
    "        f.write(\"\\\\usepackage[utf8]{inputenc}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsmath}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsfonts}\\n\") \n",
    "        f.write(\"\\\\usepackage{amssymb}\\n\")\n",
    "        f.write(\"\\\\usepackage{graphicx}\\n\")\n",
    "        f.write(\"\\\\usepackage{hyperref}\\n\")\n",
    "        f.write(\"\\\\title{recreation_v8_v1}\\n\")\n",
    "        f.write(\"\\\\author{Tien Dung Doan}\\n\")\n",
    "        f.write(\"\\\\begin{document}\\n\")\n",
    "        f.write(\"\\\\maketitle\\n\")\n",
    "        i = 1\n",
    "        for qid, data in input_data.items():\n",
    "            word_problem = data.get(\"response_dict\", {}).get(\"word_problem\", \"\")\n",
    "            solution = data.get(\"response_dict\", {}).get(\"solution\", \"\")\n",
    "            kc1 = data.get(\"Primary_kc\", \"\")\n",
    "            kc2 = data.get(\"Secondary_kc\", \"\")\n",
    "            topic = data.get(\"Topic\", \"\")\n",
    "            grade = data.get(\"Grade\", \"\")\n",
    "            \n",
    "            safe_qid = qid.replace(\"_\", \"\\\\_\")\n",
    "            f.write(f\"\\\\section*{{Question {i}}}\\n\")\n",
    "            i += 1\n",
    "\n",
    "            # Metadata\n",
    "            f.write(\"\\\\textbf{Metadata}\\n\\n\")\n",
    "            f.write(f\"\\\\begin{{itemize}}\\n\")\n",
    "            f.write(f\"  \\\\item Question ID: {safe_qid}\\n\")\n",
    "            f.write(f\"  \\\\item Primary KC: {kc1}\\n\")\n",
    "            f.write(f\"  \\\\item Secondary KC: {kc2}\\n\")\n",
    "            f.write(f\"  \\\\item Topic: {topic}\\n\")\n",
    "            f.write(f\"  \\\\item Grade: {grade}\\n\")\n",
    "            f.write(f\"\\\\end{{itemize}}\\n\\n\")\n",
    "\n",
    "            # Question\n",
    "            f.write(\"\\\\textbf{Question}\\n\\n\")\n",
    "            f.write(f\"{word_problem}\\n\\n\")\n",
    "\n",
    "            # Solution\n",
    "            f.write(\"\\\\textbf{Solution}\\n\\n\")\n",
    "            f.write(f\"{solution}\\n\\n\")\n",
    "\n",
    "        f.write(\"\\\\end{document}\\n\")\n",
    "\n",
    "def convert_json_into_tex_question(input_path, output_path, title):\n",
    "    if isinstance(input_path, (str, Path)) and Path(input_path).exists():\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            input_data = json.load(f)\n",
    "    else:\n",
    "        input_data = input_path  # assume it's already a dictionary\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\\\documentclass{article}\\n\")\n",
    "        f.write(\"\\\\usepackage[utf8]{inputenc}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsmath}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsfonts}\\n\")\n",
    "        f.write(\"\\\\usepackage{amssymb}\\n\")\n",
    "        f.write(\"\\\\usepackage{graphicx}\\n\")\n",
    "        f.write(\"\\\\usepackage{hyperref}\\n\")\n",
    "        f.write(f\"\\\\title{ {title} }\\n\")\n",
    "        f.write(\"\\\\author{Tien Dung Doan}\\n\")\n",
    "        f.write(\"\\\\begin{document}\\n\")\n",
    "        f.write(\"\\\\maketitle\\n\")\n",
    "        i = 1\n",
    "        for qid, data in input_data.items():\n",
    "            word_problem = data.get(\"response_dict\", {}).get(\"word_problem\", \"\")\n",
    "            kc1 = data.get(\"Primary_kc\", \"\")\n",
    "            kc2 = data.get(\"Secondary_kc\", \"\")\n",
    "            topic = data.get(\"Topic\", \"\")\n",
    "            grade = data.get(\"Grade\", \"\")\n",
    "            \n",
    "            safe_qid = qid.replace(\"_\", \"\\\\_\")\n",
    "            f.write(f\"\\\\section*{{Question {i}}}\\n\")\n",
    "            i += 1\n",
    "\n",
    "            # Metadata\n",
    "            f.write(\"\\\\textbf{Metadata}\\n\\n\")\n",
    "            f.write(f\"\\\\begin{{itemize}}\\n\")\n",
    "            f.write(f\"  \\\\item Question ID: {safe_qid}\\n\")\n",
    "            f.write(f\"  \\\\item Primary KC: {kc1}\\n\")\n",
    "            f.write(f\"  \\\\item Secondary KC: {kc2}\\n\")\n",
    "            f.write(f\"  \\\\item Topic: {topic}\\n\")\n",
    "            f.write(f\"  \\\\item Grade: {grade}\\n\")\n",
    "            f.write(f\"\\\\end{{itemize}}\\n\\n\")\n",
    "\n",
    "            # Question\n",
    "            f.write(\"\\\\textbf{Question}\\n\\n\")\n",
    "            f.write(f\"{word_problem}\\n\\n\")\n",
    "\n",
    "        f.write(\"\\\\end{document}\\n\")\n",
    "\n",
    "def convert_json_into_tex_solution(input_path, output_path, title):\n",
    "    if isinstance(input_path, (str, Path)) and Path(input_path).exists():\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            input_data = json.load(f)\n",
    "    else:\n",
    "        input_data = input_path  # assume it's already a dictionary\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\\\documentclass{article}\\n\")\n",
    "        f.write(\"\\\\usepackage[utf8]{inputenc}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsmath}\\n\")\n",
    "        f.write(\"\\\\usepackage{amsfonts}\\n\")\n",
    "        f.write(\"\\\\usepackage{amssymb}\\n\")\n",
    "        f.write(\"\\\\usepackage{graphicx}\\n\")\n",
    "        f.write(\"\\\\usepackage{hyperref}\\n\")\n",
    "        f.write(f\"\\\\title{ {title} }\\n\")\n",
    "        f.write(\"\\\\author{Tien Dung Doan}\\n\")\n",
    "        f.write(\"\\\\begin{document}\\n\")\n",
    "        f.write(\"\\\\maketitle\\n\")\n",
    "        i = 1\n",
    "        for qid, data in input_data.items():\n",
    "            word_problem = data.get(\"response_dict\", {}).get(\"word_problem\", \"\")\n",
    "            solution = data.get(\"response_dict\", {}).get(\"solution\", \"\")\n",
    "            kc1 = data.get(\"Primary_kc\", \"\")\n",
    "            kc2 = data.get(\"Secondary_kc\", \"\")\n",
    "            topic = data.get(\"Topic\", \"\")\n",
    "            grade = data.get(\"Grade\", \"\")\n",
    "            \n",
    "            safe_qid = qid.replace(\"_\", \"\\\\_\")\n",
    "            f.write(f\"\\\\section*{{Question {i}}}\\n\")\n",
    "            i += 1\n",
    "\n",
    "            # Metadata\n",
    "            f.write(\"\\\\textbf{Metadata}\\n\\n\")\n",
    "            f.write(f\"\\\\begin{{itemize}}\\n\")\n",
    "            f.write(f\"  \\\\item Question ID: {safe_qid}\\n\")\n",
    "            f.write(f\"  \\\\item Primary KC: {kc1}\\n\")\n",
    "            f.write(f\"  \\\\item Secondary KC: {kc2}\\n\")\n",
    "            f.write(f\"  \\\\item Topic: {topic}\\n\")\n",
    "            f.write(f\"  \\\\item Grade: {grade}\\n\")\n",
    "            f.write(f\"\\\\end{{itemize}}\\n\\n\")\n",
    "\n",
    "            # Solution\n",
    "            f.write(\"\\\\textbf{Solution}\\n\\n\")\n",
    "            f.write(f\"{solution}\\n\\n\")\n",
    "\n",
    "        f.write(\"\\\\end{document}\\n\")\n",
    "\n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_into_tex(r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1.json\", r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_into_tex_question(r\"Data\\Generated_questions\\GPT4.1\\50_hardest\\50_hardest_services_without_solution_v1.json\", r\"Data\\Generated_questions\\GPT4.1\\50_hardest\\50_hardest_services_without_solution_v1.tex\", \"50 hardest services without solution v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"Data\\Generated_questions\\GPT4.1\\All\\recreation_v8_v1_split.json\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    input_data = json.load(f)\n",
    "for name, data in input_data.items():\n",
    "    output_question_path = fr\"Data/Generated_questions/GPT4.1/All/{name}_question_recreation_v8_v1.tex\"\n",
    "    output_solution_path = fr\"Data/Generated_questions/GPT4.1/All/{name}_solution_recreation_v8_v1.tex\"\n",
    "\n",
    "    title_question = f\"{name} Questions recreation v8 v1\"\n",
    "    title_solution = f\"{name} Solutions recreation v8 v1\"\n",
    "\n",
    "    convert_json_into_tex_question(data, output_question_path, title_question)\n",
    "    convert_json_into_tex_solution(data, output_solution_path, title_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"Data\\Generated_questions\\GPT4.1\\All\\services\\services_v8_v1_split.json\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    input_data = json.load(f)\n",
    "for name, data in input_data.items():\n",
    "    output_question_path = fr\"Data\\Generated_questions\\GPT4.1\\All\\services\\{name}_question_services_v8_v1.tex\"\n",
    "    output_solution_path = fr\"Data\\Generated_questions\\GPT4.1\\All\\services\\{name}_solution_services_v8_v1.tex\"\n",
    "\n",
    "    title_question = f\"{name} Questions services v8 v1\"\n",
    "    title_solution = f\"{name} Solutions services v8 v1\"\n",
    "\n",
    "    convert_json_into_tex_question(data, output_question_path, title_question)\n",
    "    convert_json_into_tex_solution(data, output_solution_path, title_solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
